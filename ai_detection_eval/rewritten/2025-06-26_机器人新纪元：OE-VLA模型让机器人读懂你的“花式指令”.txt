标题：机器人新纪元：OE-VLA模型让机器人读懂你的“花式指令”


近年来，视觉-语言-动作（VLA）模型在机器人领域崭露头角。
这类模型利用在海量互联网数据上训练的视觉-语言基础模型，通过端到端神经网络，直接从视觉观察和人类指令生成机器人动作。
然而，现有的VLA模型通常只接受单一形式的语言指令，这限制了它们在开放式人机交互中的应用。
例如，用户可能希望机器人检索图像中展示的物体，遵循白板上书写的指令，或模仿视频中演示的行为，而不仅仅依赖于语言描述。
为了弥补这一差距，来自西湖大学和浙江大学的研究者们提出了OE-VLA，旨在探索VLA模型处理开放式多模态指令的潜力。
**研究动机与背景**

当前VLA模型主要依赖纯语言指令来引导机器人完成任务，但在日常生活中，人类的指令形式是多种多样的。
例如，可以是白板上的光学指令、文本和图像的组合、一段演示视频，甚至仅仅是一个目标图像。
为了提升人机交互的自然性和灵活性，研究者们希望VLA模型能够理解和执行这些“花式指令”。
**方法与技术亮点**

OE-VLA的核心在于其能够处理多种形式的输入，包括：
*   **视觉对象指定 (VOS)**：
用图像代替文字描述物体，例如“抓取<img1>并放入<img2>”。
*   **光学指令跟随 (OIF)**：机器人需要理解图像中的指令，例如白板上的手写文字。
*   **视觉目标达成 (VGR)**：机器人通过对比当前状态和目标图像，采取行动以达到目标状态。
*   **视频演示学习 (VDL)**：机器人观看一段简短的视频，学习并模仿视频中的动作。
为了实现这一目标，研究者们设计了一个包含视觉编码器、MLP投影器和LLM骨干网络的统一神经架构。
该架构以LLaVA-Next-Interleave作为基础模型，因为它能够处理自由形式的多图像输入，并在相关基准测试中表现出色。
为了训练OE-VLA，研究者们还提出了一种两阶段课程学习策略：
1.
**多图像对齐**：增强模型对物体之间空间关系的感知能力。
2.
**开放式指令调优**：使用构建的机器人操作数据集，对模型进行微调，使其能够处理各种开放式任务规范。
**主要发现与成果**

实验结果表明，OE-VLA不仅在遵循语言指令方面与传统VLA模型表现相当，
而且在处理多种自由形式的人类提示方面也取得了令人印象深刻的结果。
研究者们还构建了两个新的基准测试集OE-CALVIN base和OE-CALVIN hard，以评估OE-VLA在开放式指令下的性能。
实验结果表明，OE-VLA在这些基准测试集上表现出色，证明了其处理多模态指令的能力。
**意义与应用前景**

OE-VLA的提出，极大地拓展了VLA模型在各种日常场景中的应用，并促进了人机交互。
未来，我们可以期待看到更多能够理解和执行“花式指令”的机器人，它们将更加智能、灵活，并能够更好地服务于人类。
标签：#机器人 #视觉语言动作模型 #多模态学习 #人机交互 #AI