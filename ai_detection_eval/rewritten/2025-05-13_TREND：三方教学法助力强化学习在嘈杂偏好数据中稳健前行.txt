标题：TREND：三方教学法助力强化学习在嘈杂偏好数据中稳健前行


在强化学习中，如何让AI智能体学会完成复杂任务一直是个难题。
传统方法需要人为设计奖励函数，这既费时又容易出错。
偏好强化学习（PbRL）通过让人类或视觉语言模型（VLM）对智能体的行为片段进行偏好排序，
从而避免了手动设计奖励函数的麻烦。
然而，这些偏好数据往往包含噪声，严重影响学习效果。
本文介绍了一种名为TREND的新框架，它巧妙地结合了少量专家演示和三方教学策略，有效降低了噪声的影响，
显著提升了智能体在嘈杂环境下的学习能力。
**研究动机与背景**

PbRL的核心思想是让智能体从人类或VLM提供的偏好反馈中学习。
但现实中，人类反馈容易受到主观偏见的影响，而VLM则可能难以理解复杂的视觉场景和任务目标，导致偏好标签出现噪声。
即使是很小的噪声比例（如10%）也会显著降低PbRL的性能。
因此，开发一种能够有效处理噪声偏好标签的鲁棒PbRL方法至关重要。
**方法与技术亮点**

TREND的核心在于其三方教学策略。
该策略同时训练三个奖励模型，每个模型都将损失较小的偏好对视为有用的知识，并将其“教”给其他两个模型，
从而更新它们的参数。
这种方法类似于三个互相学习的学生，通过互相分享“干净”的样本来提高整体的学习效果。
与简单地平均多个奖励模型的预测结果相比，TREND的三方教学策略具有以下优势：每个模型都能独立发展样本选择的“专业知识”，
增强了对噪声的鲁棒性；“干净”样本的定义是动态学习的，而不是固定的。
此外，为了进一步应对高噪声环境，TREND还整合了少量专家演示。
这些演示数据作为预训练和在线PbRL适应的初始指导，确保了训练数据的质量，并为策略学习提供了一个良好的起点。
**主要发现与成果**

研究人员在Meta-world的机器人操作任务上评估了TREND的性能。
实验结果表明，即使在高达40%的噪声水平下，TREND也能达到高达90%的成功率，显著优于现有的PbRL方法。
例如，在Button-Press任务上，TREND的成功率比基线方法提高了近40%；在Drawer-Open任务上，提高了60%；
在Hammer任务上，提高了70%。
即使使用VLM生成的嘈杂偏好标签，TREND在Drawer-Open任务上的成功率也提高了40%以上。
**意义与应用前景**

TREND的成功表明，通过结合三方教学策略和少量专家演示，可以有效解决PbRL中噪声偏好标签的问题。
这项研究成果对于推动PbRL在实际机器人应用中的应用具有重要意义，尤其是在人类反馈不总是可靠或VLM难以准确理解任务的情况下。
未来，研究人员可以进一步探索如何利用TREND框架来处理更复杂的任务和更高噪声水平的偏好数据。
标签：#强化学习 #偏好学习 #机器人 #噪声鲁棒性 #三方教学