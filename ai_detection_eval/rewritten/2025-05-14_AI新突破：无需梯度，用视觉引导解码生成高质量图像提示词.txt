标题：AI新突破：无需梯度，用视觉引导解码生成高质量图像提示词


近年来，文本到图像的生成模型，如DALL-E和Stable Diffusion，
在视觉内容创作领域取得了革命性的进展。
然而，如何设计有效的文本提示词来引导这些模型仍然是一个挑战，往往需要大量的尝试和错误。
为了解决这个问题，本文提出了一种名为“视觉引导解码”（VGD）的全新方法，它无需梯度计算，
而是利用大型语言模型（LLMs）和CLIP模型的视觉引导能力，生成连贯且语义对齐的提示词。
研究的动机在于现有提示词反演方法，无论是软提示还是硬提示，都存在局限性。
软提示虽然能生成控制图像生成的向量，但缺乏可解释性，用户难以理解和修改。
硬提示虽然生成人类可读的文本，但往往包含大量无意义的词语，影响生成图像的质量。
VGD旨在生成完全可解释的提示词，从而减少文本到图像生成过程中不必要的尝试。
VGD的核心思想是利用LLM强大的文本生成能力，生成符合人类语言习惯的提示词。
同时，VGD使用CLIP模型来确保生成的提示词与用户指定的视觉概念对齐。
具体来说，VGD通过token-by-token的方式生成提示词，每生成一个token，都会计算其与目标图像的CLIP相似度，
并结合LLM的语言模型概率，选择最优的token。
这种方法确保了生成的提示词既语义相关，又易于理解。
论文的主要发现是，VGD在生成可理解和上下文相关的提示词方面优于现有的提示词反演技术。
实验结果表明，VGD在多个数据集上取得了最先进的性能，并且具有良好的泛化能力，可以与不同的LLM和图像生成模型结合使用。
例如，VGD可以与LLaMA2、LLaMA3和Mistral等LLM以及DALL-E 2、MidJourney和Stable Diffusion等图像生成模型无缝衔接。
VGD的意义在于它提供了一种更直观和可控的文本到图像生成方式。
用户可以通过修改VGD生成的提示词，轻松地改变图像的风格、内容和细节。
此外，VGD无需额外的训练，易于集成到现有的聊天界面中，为用户提供更便捷的图像生成服务。
VGD的应用前景广阔，包括广告、个性化媒体、设计原型等领域。
标签：#人工智能 #图像生成 #自然语言处理 #提示词工程 #深度学习