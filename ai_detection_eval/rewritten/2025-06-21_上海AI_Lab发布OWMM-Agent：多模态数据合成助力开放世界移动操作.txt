标题：上海AI Lab发布OWMM-Agent：多模态数据合成助力开放世界移动操作


近年来，导航、操作和视觉模型的飞速发展，
使得移动操作机器人能够在许多特定任务中表现出色。
然而，在开放、未知的环境中，如何让机器人理解自然语言指令，并完成复杂的移动操作（OWMM）任务，仍然是一个巨大的挑战。
上海AI Lab的研究团队发布了一篇名为 "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis" 的论文，
提出了一种新颖的多模态智能体架构，并利用智能体数据合成流程来提升模型性能，为解决这一难题带来了新的思路。
**研究动机与背景**

OWMM任务的难点在于，机器人需要具备全局场景理解能力，能够根据自然语言指令和自身状态做出决策，
并将高层次的决策转化为低层次的机器人控制指令。
以往的方法通常依赖于2D语义地图或3D语义场，但这些方法受限于嵌入模型的容量，难以处理复杂的指令，且需要耗时的3D重建。
而大型语言模型（LLM）和视觉语言模型（VLM）的出现，为解决这些问题提供了新的可能。
**方法与技术亮点**

为了充分利用VLM的能力，研究团队提出了OWMM-Agent框架，
将OWMM任务转化为一个多轮、多图像、多模态的推理问题。
该框架包含以下几个关键组成部分：

1.
**多模态记忆**：维护多视角场景帧和智能体状态，用于决策。
2.
**多模态工具**：利用VLM生成思维链（CoT）推理过程、跟踪智能体状态，并生成带有坐标的多模态动作。
3.
**坐标规划器**：根据VLM生成的坐标，控制机器人进行导航和操作。
此外，为了解决VLM在具身智能任务中存在的领域迁移问题，研究团队还引入了一个智能体数据合成流程。
该流程利用预定义的任务序列模板和模拟环境中的真实世界表示，生成大规模的、指令驱动的训练数据，
使VLM能够更好地跟踪自身状态、推理多视角观察结果，并生成基于全局场景和智能体自身状态的多模态动作。
**主要发现与成果**

实验结果表明，OWMM-Agent在模拟环境中取得了SOTA的性能，
并且在真实世界中也展现出了强大的零样本泛化能力。
具体来说，OWMM-VLM模型在Fetch机器人上的动作生成成功率达到了90%，即使该模型仅在模拟数据上进行了微调。
**意义与应用前景**

OWMM-Agent的提出，为开放世界移动操作任务提供了一个新的解决方案。
该框架利用VLM强大的场景理解和推理能力，结合智能体数据合成流程，有效地解决了领域迁移问题，并实现了高精度的机器人控制。
这项研究的成果，有望推动通用家庭助理机器人的发展，使其能够更好地服务于人类的生活。
标签：#机器人 #视觉语言模型 #多模态学习 #具身智能 #开放世界