标题：NEXUS SUM：多智能体LLM架构革新长篇叙事摘要


在自然语言处理领域，长篇叙事文本的自动摘要一直是一项极具挑战性的任务。
近日，一项名为NEXUS SUM的研究提出了一种新颖的层级式多智能体LLM框架，
旨在解决现有大型语言模型（LLM）在处理书籍、电影剧本等长文本时遇到的难题，无需微调即可实现高质量的叙事摘要。
这项研究的潜在价值在于为各类型故事文本的理解和内容提取提供了一种更高效、更具扩展性的方法。
该研究的核心在于两个创新点。
首先是“对话到描述的转换”：一种针对叙事文本的预处理方法，将人物对话和描述性文本统一转换为结构化散文，
从而提高文本的连贯性。
这种转换减少了多角色对话带来的信息碎片化，使得LLM更容易捕捉到故事的主线和人物关系。
其次是“层级式多LLM摘要”：一个结构化的摘要流程，优化了分块处理过程，并精确控制输出长度，以确保摘要的准确性和高质量。
该流程通过分而治之的策略，将长文本分割成易于处理的片段，再由多个LLM智能体协同工作，逐步提炼和压缩信息。
NEXUS SUM框架包含三个主要阶段。
首先，预处理智能体将对话转换为结构化的叙述性文字，减少信息碎片化，提高连贯性。
其次，叙事摘要智能体生成初步的综合摘要，保留关键情节和人物互动。
最后，迭代压缩智能体通过可控的压缩动态减少摘要长度，确保关键信息得到保留，同时满足长度约束。
通过将长输入分割成易于管理的块，并在多个LLM智能体上应用分层处理，NEXUS SUM确保了高保真摘要，并具有可扩展的长度控制。
研究结果表明，NEXUS SUM在多个长篇叙事数据集上取得了显著的性能提升，在BERTScore (F1) 指标上实现了高达30.0%的改进，
确立了新的技术水平。
这一成果不仅验证了多智能体LLM在处理长文本内容方面的有效性，也为结构化摘要提供了一种可扩展的方法，
适用于各种故事讲述领域。
NEXUS SUM的成功，为AI驱动的故事创作、个性化摘要以及对话式AI等领域开辟了新的可能性。
标签：#自然语言处理 #长文本摘要 #多智能体 #大型语言模型 #叙事理解