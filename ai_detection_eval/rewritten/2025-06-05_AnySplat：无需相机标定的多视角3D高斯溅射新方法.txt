标题：AnySplat：无需相机标定的多视角3D高斯溅射新方法


近年来，利用多视角图像重建3D场景的技术取得了显著进展。
然而，传统方法通常依赖于精确的相机标定和耗时的逐场景优化，这限制了其在实际应用中的普及。
针对这一问题，一篇名为AnySplat的论文提出了一种新颖的feed-forward网络，能够从未经标定的图像集合中直接合成高质量的新视角图像，
极大地简化了3D重建流程。
**研究动机与背景**：
现有的多视角重建和神经渲染方法，如NeRF及其扩展，虽然能生成高质量的渲染结果，
但需要精确的相机位姿，并且计算成本高昂。
而AnySplat旨在解决这些问题，它无需相机位姿信息，仅通过单次前向传播即可预测场景的3D高斯基元以及每个输入图像的相机内外参数，
从而实现快速且鲁棒的3D重建。
**方法与技术亮点**：
AnySplat的核心在于其feed-forward网络结构。
该网络包含一个几何Transformer，用于将多视角图像编码成高维特征，然后解码成3D高斯参数和相机位姿。
为了提高效率，论文还引入了一个可微体素化模块，将像素级高斯基元合并为体素级高斯，从而减少了30%-70%的冗余基元，
同时保持了渲染质量。
此外，该方法还设计了一种自监督知识蒸馏流程，从预训练的VGGT模型中提取相机和几何先验知识，作为外部监督，
从而无需任何3D SfM或MVS监督即可进行训练。
**主要发现与成果**：
实验结果表明，AnySplat在稀疏和稠密视角场景下均能达到与需要位姿信息的方法相媲美的质量，
并且优于现有的无位姿方法。
更重要的是，AnySplat显著降低了渲染延迟，使得在无约束的拍摄环境下实现实时新视角合成成为可能。
在多个数据集上的实验结果表明，AnySplat在zero-shot泛化性能上表现出色，能够处理各种几何和外观变化。
**意义与应用前景**：
AnySplat的出现，降低了3D重建的技术门槛，
使得从随意拍摄的多视角图像中快速生成高质量3D模型成为可能。
这项技术在虚拟现实、增强现实、机器人导航等领域具有广阔的应用前景。
例如，用户可以使用手机随意拍摄一组照片，即可快速生成可交互的3D场景，用于游戏开发、室内设计等应用。
标签：#三维重建 #新视角合成 #高斯溅射 #无位姿估计 #深度学习