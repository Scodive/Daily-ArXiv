标题：LLM Agent不确定性量化：传统方法亟待重新评估


大型语言模型（LLM）在生成内容时常出现“幻觉”，即产生不正确的信息。
虽然完全避免LLM产生幻觉已被证明是不可能的，但量化LLM的不确定性，从而提高输出的可信度变得至关重要。
传统的不确定性量化方法通常将不确定性分为“偶然不确定性”（aleatoric uncertainty）和“认知不确定性”（epistemic uncertainty）两类。
然而，本文指出，这种二分法在LLM Agent与用户交互的开放环境中存在局限性，需要更丰富的不确定性表达方式。
**研究动机与背景**

传统的偶然不确定性指的是数据本身固有的不确定性，无法通过增加数据或改进模型来消除。
认知不确定性则源于模型对某些知识的缺乏，可以通过更多数据或更好的训练来降低。
然而，在LLM Agent与用户的多轮对话中，信息的缺失、任务的不明确性以及交互的动态性使得这种二分法变得模糊。
**核心内容解读**

该论文的核心观点是，传统的偶然不确定性和认知不确定性的划分在LLM Agent的交互场景中不再适用。
文中通过具体例子和对现有文献的回顾，揭示了现有不确定性定义之间的矛盾。
例如，对于一个二元分类问题，如果模型只认为两个参数是合理的，那么这种状态是最大认知不确定性还是最小认知不确定性？
不同的理论基础会得出截然相反的结论。
此外，LLM Agent可以通过提问来获取更多信息，从而将原本看似偶然的不确定性转化为可降低的认知不确定性，反之亦然。
**方法与技术亮点**

针对LLM Agent交互中的不确定性，论文提出了三个新的研究方向：

1.
**欠规范不确定性（Underspecification uncertainties）**：用户在初始阶段可能没有提供所有必要信息或明确定义任务，
导致模型对任务和上下文理解不足。
2.
**交互式学习（Interactive learning）**：LLM Agent可以通过主动提问来澄清用户的意图，减少欠规范不确定性。
3.
**输出不确定性（Output uncertainties）**：利用丰富的语言和语音空间，以更自然、更具解释性的方式表达不确定性，
而不仅仅是数字概率。
**意义与应用前景**

通过探索这些新的研究方向，可以构建更透明、更值得信赖、更易于理解的LLM Agent。
例如，当用户的问题不够明确时，Agent可以主动询问，而不是直接给出一个可能不准确的答案。
此外，Agent还可以用自然语言解释其不确定性的来源，以及哪些信息可以帮助消除不确定性。
这些改进将使LLM Agent在实际应用中更加可靠和有用。
标签：#大型语言模型 #不确定性量化 #人机交互