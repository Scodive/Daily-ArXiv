标题：揭秘大语言模型微调的“学习动力学”：理解幻觉与DPO的奥秘


大型语言模型（LLM）在各种任务中展现出惊人的能力，
但如何确保它们遵循人类指令并与人类偏好对齐，仍然是一个重要的研究课题。
本文解读一篇ICLR 2025会议论文，该论文从“学习动力学”的角度，深入研究了LLM在微调过程中的行为，
为理解幻觉现象和优化微调算法提供了新的视角。
**研究动机与背景：**
传统的LLM微调分析主要关注训练目标、训练结束时的模型状态，或者与强化学习的关系。
这篇论文另辟蹊径，从动态的角度观察LLM的演变，通过分析模型在学习特定训练样本时，如何影响其对其他样本的预测，
来理解LLM微调的内在机制。
**方法与技术亮点：**
论文的核心在于构建了一个“学习动力学”框架，
将LLM微调过程中模型预测的变化分解为三个关键因素，并用数学公式精确表达。
这个框架可以统一解释各种微调算法，包括监督微调（SFT）、直接偏好优化（DPO）及其变体，甚至基于强化学习的方法。
* **影响分解：** 模型预测的改变被分解为三个部分：
模型当前预测概率的影响（At）、神经正切核（NTK）度量的输入样本相似度（Kt）、以及损失函数决定的能量和方向（Gt）。
* **“挤压效应”：** 论文重点提出了一个独特的“挤压效应”，解释了在离线DPO中观察到的现象：
过度训练反而会降低模型对期望输出的置信度。
这是由于梯度上升算法在具有softmax层的交叉熵损失模型上造成的。
负梯度会压低模型对几乎所有可能输出标签的预测，将概率质量转移到最可能的标签上。
* **因果掩码：** 作者通过因果掩码将自回归建模转化为多标签建模，使得分析更加简单。
**主要发现与成果：**
* **幻觉解释：** 该框架解释了为何特定类型的幻觉会在微调后得到加强，例如，
模型可能使用问题B中的短语或事实来回答问题A，或者在生成回复时重复类似的简单短语。
* **DPO的“挤压效应”：** 论文揭示了离线DPO中存在的“挤压效应”，
即负梯度会压低模型对几乎所有可能输出标签的预测，导致模型生成重复性短语。
* **改进方法：** 基于对“挤压效应”的理解，论文提出了一种简单而有效的改进方法：在SFT阶段，
不仅使用正样本，也使用负样本进行训练，以缓解“挤压效应”，从而提高对齐性能。
**意义与应用前景：**
这项研究不仅为理解LLM的微调过程提供了一个新的视角，
而且启发了一种改进对齐性能的简单方法。
对“挤压效应”的分析，也可能适用于其他将大的负梯度应用于不太可能结果的深度学习系统。
通过理解学习动力学，我们可以更好地控制LLM的行为，减少幻觉，并提升其与人类偏好对齐的能力。
标签：#大语言模型 #微调 #学习动力学 #幻觉 #DPO