标题：QuickSilver：Meta、亚马逊等机构提出全新加速LLM推理框架，最高减少39.6% FLOPs


大型语言模型（LLM）在各种自然语言处理任务中表现出色，
但其高昂的推理成本一直是部署的瓶颈。
近日，来自Manipal University Jaipur、Meta AI、Amazon AI等机构的研究者们联合提出了一种名为QuickSilver的全新推理加速框架，
旨在无需模型重训练或结构修改的情况下，通过动态token处理显著提升LLM推理效率。
这项研究有望降低LLM部署的成本和能耗，加速其在实际应用中的普及。
**研究动机与背景：**
LLM的推理过程占据了总成本的绝大部分，通常超过90%。
尽管训练效率取得了显著进展，但运行时优化仍然是关键瓶颈，尤其是在自回归解码中。
现有方法，如剪枝、量化、早退和推测解码，通常需要重训练、架构更改或破坏解码兼容性。
因此，如何在不改变模型权重和结构的前提下，实现高效的推理成为了一个重要的研究方向。
**方法与技术亮点：**
QuickSilver框架的核心在于其模块化的token级别自适应机制，它包含以下四个协同工作的组件：
1.
**动态Token Halting（DTH）**：当token的表示已经收敛时，停止对其进行计算。
通过计算每层L2更新范数来判断token是否收敛，一旦收敛，该token将不再参与后续层的计算。
2.
**KV Cache Skipping**：选择性地抑制对KV缓存的写入，以减少注意力机制的开销。
利用DTH的停止信号，跳过冗余的KV更新，从而减少内存使用和计算量。
3.
**Contextual Token Fusion**：将冗余的token合并为共享路径，以缩短序列长度。
通过计算token之间的L2距离来判断其是否冗余，并将相似的token合并为一个超级token。
4.
**Adaptive Matryoshka Quantization**：根据token的熵值自适应地调整量化精度。
为每个token计算熵值，并根据熵值大小分配不同的比特宽度，从而在保证模型性能的前提下降低内存占用和计算量。
**主要发现与成果：**
研究者们在GPT-2和Llama-2模型上，使用WikiText-103和C4数据集进行了实验。
实验结果表明，QuickSilver框架能够实现高达39.6%的FLOPs减少，而困惑度（perplexity）的退化可以忽略不计（≤0.2）。
此外，研究者们还公开了QuickSilver的实现代码，以促进该领域未来的研究。
**意义与应用前景：**
QuickSilver框架的提出，为LLM推理加速提供了一种全新的思路。
它无需修改模型结构或进行重训练，即可实现显著的性能提升，这使得它可以很容易地应用于现有的LLM部署中。
此外，QuickSilver框架的模块化设计，也使得它可以与其他推理加速技术相结合，进一步提升推理效率。
QuickSilver在降低LLM部署成本、提高用户交互体验、减少环境影响等方面具有广阔的应用前景。
标签：#大语言模型 #推理加速 #运行时优化