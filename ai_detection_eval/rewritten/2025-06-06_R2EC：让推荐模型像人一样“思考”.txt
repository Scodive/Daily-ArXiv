标题：R2EC：让推荐模型像人一样“思考”


大型语言模型（LLMs）在理解上下文和生成内容方面表现出色，
这激发了人们将LLMs应用于推荐系统。
然而，现有方法通常将LLMs作为外部推理模块，这导致资源消耗大和联合优化困难。
本文介绍了一种名为R2EC的统一大型推荐模型，它具有内在的推理能力，旨在解决这些问题。
**研究动机与背景**
当前基于LLM的推荐方法主要有两种：一是将LLMs作为强大的编码器，嵌入用户历史交互信息；
二是将物品预测转化为自回归生成物品标识符。
这些方法虽然有效，但通常将LLMs的推理能力置于次要地位，导致模型无法充分利用LLMs的潜力。
R2EC旨在通过整合推理和推荐过程，创建一个更智能、更高效的推荐系统。
**方法与技术亮点**
R2EC的核心在于其独特的模型架构和训练框架。
*   **模型架构**：R2EC在LLM主干网络的基础上，增加了一个推荐头（rec\_head），
与原有的语言建模头（lm\_head）并行。
这种设计允许模型首先自回归地生成推理tokens，然后在最后一步预测物品。
语言建模头负责生成推理tokens，而推荐头则用于对物品进行评分以进行推荐。
这种架构将生成和推荐任务集成到一个统一的模型中。
*   **训练优化**：为了训练模型的推理能力，R2EC采用了一种名为RecPO的强化学习框架。
RecPO无需人工标注的推理数据，而是通过一个融合奖励方案来模拟推理能力。
该方案结合了离散的排序奖励和连续的相似度奖励，从而优化模型的推理和推荐能力。
具体来说，RecPO从模型中抽样轨迹，这些轨迹本质上是推理-物品序列。
然后，它使用融合奖励方案为这些轨迹分配奖励。
最后，它使用一个联合推理-推荐训练目标来优化模型，从而通过推理来改进推荐。
**主要发现与成果**
实验结果表明，R2EC在三个数据集上显著优于各种传统、基于LLM和推理增强的推荐基线。
具体而言，R2EC在Hit@5指标上取得了68.67%的相对提升，在NDCG@20指标上取得了45.21%的相对提升。
这些结果验证了统一推理-推荐模型设计的有效性。
**意义与应用前景**
R2EC的成功表明，将推理能力融入大型推荐模型具有巨大的潜力。
这种方法不仅可以提高推荐的准确性，还可以提高推荐的可解释性，因为模型可以生成推理过程，解释其推荐的原因。
未来，R2EC有望应用于各种推荐场景，例如电商、新闻推荐和社交媒体。
通过不断优化推理能力，R2EC可以为用户提供更加个性化、智能化的推荐服务。
标签：#推荐系统 #大型语言模型 #强化学习 #推理 #R2EC