标题：微软&清华&北大提出RPT：用强化学习赋能大语言模型预训练，
提升推理能力


大型语言模型（LLMs）在各种任务中展现出卓越的能力，
这主要得益于在海量文本语料库上进行的可扩展的下一个token预测目标。
然而，目前强化学习（RL）在LLM训练中的应用面临可扩展性和通用性挑战。
为了解决这些问题，微软研究院、北京大学和清华大学的研究人员联合提出了强化预训练（RPT）这一新范式，
旨在弥合可扩展的自监督预训练和强化学习能力之间的差距。
该研究将下一个token预测任务重新定义为下一个token推理过程，通过强化学习来训练模型，使其能够推理并正确预测下一个token，
从而显著提高语言建模的准确性。
**研究动机与背景**：
当前RL在LLM训练中的应用，如基于人类反馈的强化学习（RLHF），虽然在对齐方面有效，
但依赖于昂贵的人类偏好数据，并且其学习到的奖励模型容易受到奖励利用的影响，限制了可扩展性。
另一方面，具有可验证奖励的强化学习（RLVR）利用客观的、基于规则的奖励，但通常受到带有可验证答案的标注数据稀缺的限制，
限制了其在特定领域微调而非通用预训练中的应用。
RPT旨在克服这些限制，充分利用大规模无标注文本数据进行通用RL。
**方法与技术亮点**：
RPT的核心思想是将下一个token预测任务转化为一个推理过程。
对于预训练语料库中的任何给定上下文，模型被激励在预测后续token之前进行推理。
它会根据其预测与来自语料库本身的真实下一个token的正确性，获得可验证的内在奖励。
这种方法将通常用于下一个token预测的大量未标注文本数据转换为用于通用RL的大型数据集，
而无需外部标注或特定领域的奖励函数。
具体来说，模型在生成最终预测token之前，需要生成一个思维链推理序列。
这个推理过程可以包括头脑风暴、自我批评和自我纠正等多种模式。
**主要发现与成果**：
实验结果表明，RPT显著提高了预测下一个token的准确性。
RPT还为后续强化微调提供了更强大的预训练基础，从而提高了最终任务的性能。
缩放曲线显示，在RPT框架下，增加训练计算量可以持续提高下一个token预测的准确性，表明其作为一种可持续的缩放策略的潜力。
在OmniMATH数据集上，RPT-14B在所有难度级别上都实现了比R1-Distill-Qwen-14B更高的next-token预测精度。
值得注意的是，它与更大的模型R1-Distill-Qwen-32B的性能相匹配。
**意义与应用前景**：
RPT的提出为大型语言模型的预训练提供了一个新的方向，
通过将下一个token预测任务转化为一个可验证的推理任务，并应用基于正确性的奖励进行强化学习，
RPT允许LLM在预训练期间利用扩展的计算来构建更强大的基础推理能力。
RPT在零样本设置中提高了数学和一般推理基准的性能，并为进一步的RL微调提供了更好的起点。
标签：#强化学习 #大型语言模型 #预训练 #推理能力 #自然语言处理