标题：大型推理模型并非“智力崩塌”？
Anthropic和Open Philanthropy 联手揭示AI评估的陷阱


Shojaee et al.
(2025) 的研究指出，大型推理模型 (LRM) 在解决复杂规划问题时会出现“准确性崩塌”。
但Anthropic和Open Philanthropy 合作的这项新研究对此提出了质疑，认为之前的发现更多是实验设计缺陷导致的假象，
而非模型本身存在根本性的推理缺陷。
这项研究通过严谨的分析，揭示了当前AI评估方法中存在的潜在陷阱，对未来的AI研究具有重要意义。
原研究的核心发现是，LRM在解决如汉诺塔、过河问题等规划谜题时，一旦问题复杂度超过一定阈值，模型的准确率就会急剧下降。
然而，新研究指出，这一现象可能与以下三个关键问题有关：首先，在汉诺塔实验中，模型在达到报告的失败点时，
实际上已经接近输出token的上限，并且模型本身也明确意识到了这一限制。
其次，原作者的自动评估框架无法区分真正的推理失败和因实际约束导致的输出截断，从而错误地评估了模型的能力。
最令人担忧的是，在过河问题测试中，当人数超过5人时，由于船的容量不足，问题本身就无解，
但模型却因为无法解决这些不可能的问题而被判定为失败。
为了验证这一观点，研究人员设计了新的实验，要求模型生成解决汉诺塔问题的函数，而不是详尽的步骤列表。
初步实验结果表明，即使在之前被报告为完全失败的汉诺塔实例上，多个模型也能表现出很高的准确率。
这表明，当模型摆脱了详尽枚举的要求后，其推理能力依然完好。
研究还通过数学公式量化了汉诺塔问题所需的token数量与问题规模之间的关系，进一步证实了token限制是导致“准确性崩塌”的重要原因。
此外，研究还指出，原作者使用“组合深度”（最少移动次数）作为复杂度指标存在缺陷，因为它混淆了机械执行和问题解决的难度。
例如，汉诺塔问题虽然需要指数级的移动次数，但每一步的决策过程却非常简单；而过河问题虽然移动次数较少，
却需要复杂的约束满足和搜索。
这解释了为什么模型可能能够执行100多次汉诺塔移动，却无法解决5步的过河问题。
这项研究强调，在评估AI推理能力时，必须谨慎设计实验，区分推理能力和输出约束，验证问题本身的 solvability，
并使用能够反映计算难度的复杂度指标。
未来的研究应该考虑使用多种解决方案表示，将算法理解与执行分离开来。
这项工作的重要结论是，我们应该关注的是如何设计更好的评估方法来区分模型的推理能力和“打字”能力，
而不是简单地断言LRM不能推理。
标签：#大型推理模型 #AI评估 #实验设计 #汉诺塔 #过河问题