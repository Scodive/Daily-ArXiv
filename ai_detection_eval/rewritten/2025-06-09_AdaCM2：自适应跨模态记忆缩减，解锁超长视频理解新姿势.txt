标题：AdaCM2：自适应跨模态记忆缩减，解锁超长视频理解新姿势


近年来，
大型语言模型（LLMs）的飞速发展极大地推动了视频理解任务的进步，特别是通过将LLMs与视觉模型相结合。
然而，现有基于LLM的模型大多局限于处理短视频。
虽然一些尝试通过提取和压缩视觉特征到固定大小的记忆中来理解长视频，但这些方法往往只关注视觉模态，
忽略了视觉和文本查询之间的关联，导致在处理复杂的问答任务时遇到困难。
为了解决长视频和复杂提示带来的挑战，研究人员提出了AdaCM2，它首次引入了一种自适应跨模态记忆缩减方法，
以自回归的方式在视频流中实现视频-文本对齐。
**研究动机与背景**

视频理解是计算机视觉和人工智能领域的一项重要任务，它涉及处理和推理视觉和文本信息。
尽管大型语言模型（LLMs）取得了显著成功，但先前的工作主要集中在短视频理解任务上，
视频时长通常在5到15秒之间。
然而，长视频理解在电影分析和视频检索等实际应用中发挥着关键作用。
随着视频长度的增加，模型需要处理的帧数迅速增长，导致巨大的内存消耗，从而阻碍了先前方法处理此类长视频。
**方法与技术亮点**

AdaCM2的核心思想是自适应地保留与文本查询最相关的关键视觉tokens。
该方法首先使用预训练的视觉编码器（Q-Former）从视频帧中提取视觉表示。
然后，为了解决巨大的内存消耗挑战并建模长期时间连接，AdaCM2引入了一种自适应跨模态注意力机制，
以逐帧方式回归地学习“查询”。
这种机制允许基于跨模态相关性在不同层保留不同数量的视觉tokens。
最后，学习到的查询被发送到LLM以生成答案。
AdaCM2的关键创新点在于：
1.
提出了一个高效的视频记忆缩减框架，基于跨模态注意力进行长期视频理解，首次促进了视频理解中视觉特征和文本提示之间的交互。
2.
提出了一个跨模态注意力模块，该模块根据与输入文本提示的相关性自适应地分析和评估视觉tokens，
从而通过动态的关键视频token保留策略实现超长视频任务。
**主要发现与成果**

在各种视频理解任务（如视频字幕、视频问答和视频分类）上的大量实验表明，
AdaCM2在多个数据集上实现了最先进的性能，同时显著降低了内存使用量。
值得注意的是，在LVU数据集的多个任务中，它实现了4.5%的改进，同时GPU内存消耗降低了高达65%。
**意义与应用前景**

AdaCM2的出现为长视频理解带来了新的突破，它不仅提高了模型的性能，还大大降低了内存需求，
使得处理超长视频成为可能。
这项研究成果在电影分析、视频检索、智能监控等领域具有广阔的应用前景。
此外，AdaCM2的框架具有良好的可扩展性，可以方便地集成到现有的视频理解模型中，进一步提升其性能。
标签：#长视频理解 #跨模态学习 #自适应记忆 #人工智能 #视频分析