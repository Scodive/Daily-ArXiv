标题：NVIDIA发布Jet-Nemotron：AI语言模型效率与精度的双重飞跃


在人工智能飞速发展的今天，大型语言模型（LLM）已成为推动技术革新的核心力量。然而，强大的性能往往伴随着巨大的计算和内存开销，尤其是在处理长文本时。近日，来自NVIDIA的研究团队推出了一款名为Jet-Nemotron的新型语言模型家族，它不仅在多项基准测试中达到了甚至超越了当前领先的全注意力模型（full-attention models）的精度，更在生成吞吐量方面实现了惊人的提升。

**Jet-Nemotron的诞生：PostNAS架构探索新范式**

Jet-Nemotron的诞生离不开一项名为“Post Neural Architecture Search”（PostNAS）的创新研究方法。传统上，设计新的语言模型架构需要从头开始预训练，成本高昂且风险巨大。PostNAS则另辟蹊径，它以一个已预训练的全注意力模型为起点，并冻结其多层感知机（MLP）权重，从而大幅降低了探索新架构的成本。这一流程包含四个关键步骤：首先，学习最优的全注意力层位置和冗余层消除；其次，选择高效的线性注意力模块；接着，设计全新的注意力模块；最后，进行硬件感知的超参数搜索。

**核心技术亮点：JetBlock与智能架构优化**

Jet-Nemotron的核心技术亮点之一是其新设计的线性注意力模块——JetBlock。该模块巧妙地将动态卷积（dynamic convolution）与线性注意力相结合，能够根据输入动态调整卷积核的特征提取模式，显著提升了模型的表达能力。与以往依赖静态卷积的方法不同，JetBlock通过一个“核生成器”来动态生成卷积核，并移除了查询（Q）和键（K）上的冗余静态卷积，进一步优化了计算效率。

此外，PostNAS流程中的“硬件感知架构搜索”尤为关键。研究发现，模型的参数量并非衡量其硬件效率的唯一标准，而是KV缓存（Key-Value cache）的大小对生成吞吐量影响更为显著。Jet-Nemotron通过这一搜索策略，在保持或提升精度的同时，优化了KV缓存的大小和模型结构，从而在实际硬件上实现了更高的生成速度。

**性能表现：速度与精度的完美结合**

实验结果令人瞩目。Jet-Nemotron-2B模型在MMLU-Pro等一系列基准测试中，不仅达到了与Qwen3、Gemma3、Llama3.2等SOTA全注意力模型相当的精度，更在生成吞吐量上实现了高达47倍的提升。即使在处理长达64K的上下文时，其预填充（prefilling）速度也有6.14倍的提升，解码速度更是达到了惊人的53.6倍。值得一提的是，即使与更大型的混合专家（MoE）模型相比，Jet-Nemotron在某些任务上也展现出更优的性能。

**未来展望：加速AI创新与应用**

Jet-Nemotron的出现，不仅为高效语言模型的研发开辟了新路径，也为AI技术的广泛应用带来了更多可能。它大幅降低了模型架构探索的门槛和风险，有望加速下一代高效语言模型的迭代与部署，为科研和产业界带来更高效、更易用的AI工具。

#语言模型 #AI效率 #NVIDIA #深度学习 #神经网络架构