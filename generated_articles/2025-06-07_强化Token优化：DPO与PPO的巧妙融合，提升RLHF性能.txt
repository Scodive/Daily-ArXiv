标题：强化Token优化：DPO与PPO的巧妙融合，提升RLHF性能


在利用人类反馈进行强化学习（RLHF）的经典框架中，近端策略优化（PPO）算法被广泛用于从稀疏的句子级别奖励中学习。尽管PPO在对齐大型语言模型方面取得了巨大成功，但其开源实现仍然存在优化空间。本文介绍了一种新的框架，将RLHF问题建模为马尔可夫决策过程（MDP），从而能够捕获细粒度的token级别信息。

**研究动机与背景**
现有的RLHF框架通常将整个响应句子视为一个动作，奖励仅在句子层面评估整体质量。然而，PPO算法是为多步强化学习问题设计的，需要对每个步骤（即每个token）进行奖励分配。传统的PPO实现中，除了为了确保微调后的LLM接近监督微调（SFT）模型而分配给每个token的正则化奖励外，学习到的句子级别奖励通常只分配给最后一个token，而其他token获得的奖励为零。这种奖励分配策略与RLHF的实际需求存在错配。

**方法与技术亮点**
为了解决上述问题，本文提出了强化Token优化（RTO）算法。RTO的核心思想是，首先从离线偏好数据中学习token级别的奖励函数，然后基于学习到的token级别奖励信号进行策略优化。RTO创新性地融合了直接偏好优化（DPO）和PPO算法。DPO原本用于处理稀疏的句子奖励，但研究发现，DPO可以提供关于响应质量的token级别特征，并能无缝地融入到后续的PPO训练阶段。

具体来说，RTO算法首先使用DPO从人类偏好数据中提取token级别的奖励信号。DPO算法通过比较不同响应的优劣，学习一个token级别的奖励函数，该函数能够反映每个token对整体响应质量的贡献。然后，RTO将这个DPO-based的token级别奖励函数分配给每个token，并使用PPO算法进行优化。PPO算法通过不断调整语言模型的策略，使得生成的token序列能够获得更高的token级别奖励，从而提升整体的响应质量。

**主要发现与成果**
实验结果表明，RTO算法在多个基准测试中都优于PPO和其他直接偏好学习算法。例如，在AlpacaEval 2基准测试中，RTO的性能比PPO提高了7.5个百分点，在Arena-Hard基准测试中提高了4.1个百分点。此外，RTO还表现出强大的数据扩展性，仅使用1/8的数据就能达到PPO的性能水平，并且随着数据量的增加，RTO的性能持续提升，而PPO的性能则早早饱和。

**意义与应用前景**
RTO算法的提出，为RLHF领域提供了一种新的思路，即通过token级别的奖励优化，可以更有效地提升语言模型的性能。RTO算法的成功，不仅验证了token级别信息的重要性，也为未来的研究方向提供了启示。例如，可以探索DPO之外的其他方法来学习token级别的奖励，也可以尝试使用PPO之外的其他强化学习算法来优化token级别的奖励。

标签：#强化学习 #人类反馈 #语言模型 #策略优化 #Token优化