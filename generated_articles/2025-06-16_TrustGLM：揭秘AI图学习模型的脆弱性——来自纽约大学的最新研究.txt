标题：TrustGLM：揭秘AI图学习模型的脆弱性——来自纽约大学的最新研究


近年来，大型语言模型（LLM）在各个领域取得了显著的成功，受此启发，研究人员开始探索将LLM应用于图数据学习，由此诞生了GraphLLM。然而，纽约大学（NYU）等机构的最新研究表明，这些看似强大的GraphLLM在面对对抗性攻击时，却显得异常脆弱。这篇发表在arXiv上的论文 "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks"，首次对GraphLLM在提示（Prompt）、文本和结构三个维度上的鲁棒性进行了全面评估，为该领域敲响了安全警钟。

**研究动机与背景**

GraphLLM通过整合节点文本属性、节点邻域结构信息以及任务特定的提示，来利用LLM的推理能力。尽管GraphLLM展现出巨大的潜力，但其在对抗扰动下的鲁棒性却鲜为人知。这对于在安全攸关的场景中部署这些模型构成了严重威胁。TrustGLM的出现正是为了填补这一空白，旨在系统性地评估GraphLLM在面对恶意攻击时的脆弱性。

**方法与技术亮点**

TrustGLM构建了一个全面的评测基准，涵盖了三种主要的攻击类型：

*   **文本攻击**：通过替换节点文本属性中的少量语义相似词语，来扰乱LLM的理解。
*   **结构攻击**：通过增删或修改图中的边，来改变节点之间的关系，从而误导模型。研究中采用了Nettack和PRBCD等先进的图结构攻击算法。
*   **提示攻击**：通过随机打乱提示模板中的候选标签顺序，来混淆模型的判断。研究创新性地提出了标签洗牌攻击、同域噪声攻击和跨域噪声攻击等新型提示攻击方式。

为了应对这些攻击，研究团队还探索了数据增强和对抗训练等防御技术，旨在提高GraphLLM的鲁棒性。

**主要发现与成果**

实验结果表明，GraphLLM对各种对抗性攻击都非常敏感。即使是微小的文本扰动，也可能导致模型性能大幅下降。此外，标准的图结构攻击方法和简单的提示标签顺序打乱，同样会对模型造成显著的影响。例如，在Cora数据集上，GraphPrompter模型在经过TextHoaxer攻击后，准确率从67.71%骤降至41.51%。

**意义与应用前景**

TrustGLM的研究成果揭示了GraphLLM在安全性方面存在的严重隐患，强调了开发更具弹性的图学习模型的重要性。该研究不仅提供了一个用于评估GraphLLM鲁棒性的标准化框架，还为开发针对性的防御策略提供了宝贵的参考。研究团队开源了TrustGLM库（[https://github.com/Palasonic5/TrustGLM.git](https://github.com/Palasonic5/TrustGLM.git)），旨在促进该领域更快速、更公平的评估和创新研究。未来，我们可以期待更多针对GraphLLM安全性的研究涌现，从而推动其在现实世界中的安全可靠应用。

标签：#图神经网络 #大语言模型 #对抗攻击 #鲁棒性评估 #安全风险