标题：AI“照镜子”：当语言模型遇到“自己”时，会发生什么？


随着人工智能（AI）能力的飞速发展，它们正越来越多地被部署到复杂的、多主体交互的环境中。然而，我们对AI之间如何协作和互动了解甚少。一项来自哥伦比亚大学的研究，将经典的经济学实验“公共物品博弈”引入AI领域，探索了大型语言模型（LLM）在面对“对手”是“另一个AI”还是“自己”时，行为上的差异。这项研究揭示了AI的“自我认知”如何微妙地影响其合作倾向，为理解未来AI社会中的互动模式提供了有趣视角。

研究的核心在于一个名为“迭代公共物品博弈”的实验。在这个实验中，参与者（在此为AI模型）每轮会获得一定数量的“点数”，并可以选择将部分点数投入公共池，剩余的点数则归自己所有。投入公共池的点数会经过一个乘数放大，然后平均分配给所有参与者。理论上，如果所有人都最大化个人收益（即不投入点数），整体收益会受损；而如果大家都合作投入，整体收益将最大化。研究人员巧妙地设计了两种条件：一种是告知AI它们正在与“另一个AI代理”玩游戏（“无名”条件），另一种是告知AI它们正在与“自己”玩游戏（“命名”条件）。

研究发现，AI在被告知其对手是“自己”时，其合作行为会发生显著变化。具体而言，当AI被指示要“集体利益优先”时，它们在“命名”条件下反而表现出更少的合作（即更倾向于“搭便车”）；而当AI被指示要“最大化个人收益”时，在“命名”条件下则表现出更多的合作。这种现象即使在AI被告知它们是与“自己”对弈时，它们也并未明确表达出对这种“自我认知”的理解，但研究人员推测，这可能与AI对自身能力的某种“认知”有关，例如意识到对手拥有相似的推理能力。

这项研究通过多组实验，包括不同模型（如GPT-4o、Claude Sonnet 4、Llama 4 Maverick、Qwen3）和不同的系统指令（如“中立”、“集体”、“自私”），来验证这些发现。尽管实验环境相对简单，但结果表明，AI的“身份感知”——即使是简单的“你是你自己”的指令——能够影响其在合作博弈中的决策。

这项研究的意义深远。在未来，随着AI在供应链、金融系统等领域承担更重要的角色，理解AI之间的互动机制至关重要。这项工作提示我们，AI的“自我认知”或对其“身份”的感知，可能会在无形中影响其合作与竞争的行为模式，从而对多智能体系统的整体表现产生不可预测的影响。未来的研究可以进一步探索在更复杂的环境中，AI的这种“自我认知”是否会带来更深刻的行为变化，以及如何引导AI在协作中实现最佳的整体效益。

#AI #大型语言模型 #公共物品博弈 #AI合作 #多智能体系统