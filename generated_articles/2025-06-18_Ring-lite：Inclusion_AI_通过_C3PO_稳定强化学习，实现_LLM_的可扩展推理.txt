标题：Ring-lite：Inclusion AI 通过 C3PO 稳定强化学习，实现 LLM 的可扩展推理


OpenAI 的 O1 系列模型和 DeepSeek-R1 的成功，展示了大规模强化学习（RL）在复杂推理任务中的巨大潜力。然而，这些模型的训练方法并未完全公开，阻碍了该领域的进一步发展。为了解决这个问题，Inclusion AI 团队提出了 Ring-lite，一种基于混合专家（MoE）的大型语言模型，通过强化学习（RL）进行优化，以实现高效且强大的推理能力。该研究旨在通过开放的训练框架、数据和模型，降低多领域推理研究的门槛。

Ring-lite 基于公开的 Ling-lite 模型构建，拥有 168 亿参数，激活参数为 27.5 亿。该模型在具有挑战性的基准测试（如 AIME、LiveCodeBench、GPQA-Diamond）上，达到了最先进（SOTA）的小规模推理模型的性能水平，同时仅激活了同类模型所需参数的三分之一。为了实现这一目标，研究团队引入了一个结合了知识提炼与强化学习的联合训练流程，并揭示了 MoE 强化学习训练中未被记录的挑战。

首先，研究团队发现强化学习训练期间存在优化不稳定性，并提出了约束上下文计算策略优化（C3PO），这是一种通过算法-系统协同设计方法来增强训练稳定性和提高计算吞吐量的新方法。C3PO 的核心在于建立一个形式化的计算预算系统，对 token 级别的梯度贡献施加显式约束，从而确保可变长度序列之间的同质梯度贡献。其次，研究团队通过实验证明，基于熵损失而非验证指标来选择用于强化学习训练的知识提炼检查点，可以在后续的强化学习训练中产生卓越的性能-效率权衡。最后，研究团队开发了一种两阶段训练范式，以协调多领域数据集成，解决混合数据集训练中出现的领域冲突。

Ring-lite 在数学、代码生成和 STEM 问题解决基准测试中，均取得了领先的性能，通过匹配或超越参数小于 100 亿的密集模型，证明了其卓越的性能。Ring-lite 在 AIME2024 和 AIME2025 上分别取得了 76.61% 和 69.11% 的成绩，在 LiveCodeBench 和 Codeforces 上分别取得了 60.66% 和 86.45% 的成绩，在 GPQA-diamond 上取得了 61.05% 的成绩。

该研究的主要贡献包括：首次开源了一个多领域 MoE 推理模型，包含开源基础设施、训练方法和训练数据集；提出了 C3PO 框架，通过固定训练 token 大小（预算）来消除响应长度差异，并选择高熵基础模型来稳定学习动态；观察到跨多个领域（数学、代码和科学）的领域间数据冲突，并引入了一种能力集成方法（分阶段训练和平衡数据混合）来解决这个问题。

Ring-lite 的发布，为更广泛的社区提供了一个完全开源的解决方案，显著降低了探索多领域推理的门槛，使研究人员和从业者能够为这个新兴领域做出贡献并从中受益。

标签：#人工智能 #强化学习 #混合专家模型 #多领域推理 #开源模型