标题：AI“火眼金睛”新技能：无需额外标注，强化学习让大模型看懂高分辨率图像


大型多模态模型（LMMs）在理解图像方面取得了长足进步，但面对高分辨率图像时，它们常常会“迷失方向”。想象一下，一张超高清的风景照，里面有无数像素点，但真正与问题相关的信息可能只是一小块区域，就像大海捞针。传统的处理方式是将整张图像切分成大量“视觉令牌”，这不仅效率低下，还可能引入大量无关信息。更糟糕的是，受限于模型处理的上下文长度，高分辨率图像往往需要被压缩或缩放，导致细节丢失。

近日，来自复旦大学和南洋理工大学的研究人员提出了一种名为“多轮基于定位的策略优化”（Multi-turn Grounding-based Policy Optimization, MGPO）的创新框架，为解决这一难题带来了新思路。这项研究的核心在于，它巧妙地借鉴了人类视觉系统的工作机制——我们并非全方位均匀聚焦，而是会根据任务需求，将视线集中在关键区域。MGPO正是要赋予LMMs这种“火眼金睛”的能力。

**核心亮点：强化学习“炼出”定位能力，告别昂贵标注**

传统方法为了让模型学会识别图像中的关键区域，需要耗费大量人力和财力去标注这些区域的坐标信息。MGPO的突破性在于，它证明了LMMs可以在强化学习（RL）训练过程中，仅凭对最终答案正确与否的二元奖励（对/错），就能“自主”学习到精准的视觉定位能力。这意味着，模型在解决问题的过程中，会自己学会“看哪里”。

具体来说，MGPO通过一个多轮对话的框架来引导模型。当模型接收到一张高分辨率图像和一个问题时，它首先会预测出与问题相关的关键区域的坐标。接着，系统会根据这些坐标“裁剪”出相应的子图像，并将其作为新的输入提供给模型。在后续的对话轮次中，模型可以结合原始图像和裁剪出的子图像信息，进行更精细化的推理，最终给出答案。

为了解决模型在训练初期“不知道该看哪里”的“冷启动”问题，研究人员设计了一个巧妙的两轮对话模板。第一轮，模型被引导输出关键区域的坐标；第二轮，模型则在获得裁剪后的子图像后，进行问题解答。这种设计不仅避免了对额外标注数据的依赖，还通过限制策略损失的计算仅在多轮对话的输出上，保证了训练的稳定性。

**成果斐然：超越SFT和GRPO，媲美顶尖模型**

实验结果令人振奋。在标准视觉问答数据集上进行训练，MGPO相比于仅进行监督微调（SFT）和另一种强化学习方法GRPO，显著提升了模型的视觉定位能力。在“分布内”的MME-Realworld数据集上，MGPO带来了5.4%的性能提升；而在更具挑战性的“分布外”V* Bench数据集上，性能提升高达5.2%。

更令人瞩目的是，经过MGPO后训练的Qwen2.5-VL-7B模型，仅使用了21,000个样本，就在V* Bench数据集上超越了OpenAI的o1和GPT-4o模型。这充分证明了MGPO在提升模型处理高分辨率图像能力方面的强大潜力，即使在数据有限的情况下也能取得优异表现。

**意义深远：解锁高分辨率视觉理解新篇章**

MGPO的出现，不仅为解决LMMs在高分辨率图像处理中的瓶颈提供了有效的解决方案，更重要的是，它展示了一种无需昂贵标注数据即可培养模型关键视觉推理能力的新范式。这项研究有望推动多模态大模型在自动驾驶、医疗影像分析、遥感图像解读等需要精细视觉理解的领域，迈向新的高度。

#多模态大模型 #计算机视觉 #强化学习 #AI技术 #高分辨率图像