标题：告别“奖励黑客”：清华等机构提出更稳定的AI图像生成新方法与评测基准


近日，一项来自清华大学、复旦大学及腾讯等机构的研究，在人工智能生成图像（Text-to-Image, T2I）领域带来了重要进展。论文《PREF-GRPO: PAIRWISE PREFERENCE REWARD-BASED GRPO FOR STABLE TEXT-TO-IMAGE REINFORCEMENT LEARNING》不仅提出了一种名为PREF-GRPO的新型强化学习方法，以解决当前T2I模型训练中普遍存在的“奖励黑客”（reward hacking）问题，还构建了一个名为UNIGENBENCH的全新、更精细化的T2I评测基准。

**“奖励黑客”：AI图像生成中的棘手难题**

在AI生成图像的过程中，研究人员常利用强化学习（Reinforcement Learning, RL）来优化模型，使其生成的图像更符合人类的偏好。常用的方法是基于“点对点奖励”（pointwise reward）模型，即为每张生成的图像打分，然后通过分数差异来指导模型学习。然而，这种方法容易陷入“奖励黑客”的陷阱：模型为了追求高分，可能过度优化微小的、甚至无关紧要的奖励差异，导致生成的图像质量实际上在下降，但分数却在升高。论文指出，这源于“幻觉优势”（illusory advantage），即点对点评分模型在区分相似图像时分数差异极小，经过归一化后这些微小差异被不成比例地放大，误导了模型。

**PREF-GRPO：从“分数最大化”到“偏好拟合”**

为解决这一难题，研究团队提出了PREF-GRPO（Pairwise Preference Reward-based GRPO）。其核心创新在于，将优化目标从传统的“奖励分数最大化”转变为“成对偏好拟合”（pairwise preference fitting）。具体来说，PREF-GRPO会比较一组生成图像中的所有图像对，判断哪一张更好，然后根据每张图像的“胜率”来计算奖励信号。这种方法有三大优势：

1.  **放大奖励方差**：通过成对比较，高质量图像的胜率趋近于1，低质量图像趋近于0，从而产生更具区分度和稳定性的奖励信号，有效避免了“幻觉优势”。
2.  **增强鲁棒性**：专注于图像间的相对排序而非绝对分数，减少了模型对微小分数增益的过度优化，从而缓解了“奖励黑客”问题。
3.  **对齐人类偏好**：成对比较更符合人类评估图像的直观方式，能更准确地捕捉细微的质量差异。

实验结果表明，PREF-GRPO能够更精确地识别图像质量的细微差别，提供更稳定、更有方向性的优势信号，显著提升了训练的稳定性，并有效解决了“奖励黑客”现象。

**UNIGENBENCH：更精细的T2I评测新标准**

除了方法上的创新，研究团队还构建了UNIGENBENCH，一个旨在提供更全面、更精细化评估的T2I基准。现有基准往往只关注宏观的评估维度，而UNIGENBENCH则将10个主要评估维度细化为27个子维度，涵盖了如逻辑推理、构图关系、属性绑定等更细致的方面。该基准还利用大型多模态语言模型（MLLM）自动化了提示词生成和图像评估流程，大大提高了评估效率和准确性。通过对多款主流T2I模型的测试，UNIGENBENCH揭示了它们在不同维度上的优势与不足，尤其是在逻辑推理和文本渲染等复杂任务上，仍有较大的提升空间。

这项研究不仅为AI图像生成带来了更稳定、更可靠的训练方法，也为未来T2I模型的发展和评估提供了更科学、更全面的工具。

#AI图像生成 #强化学习 #文本到图像 #深度学习 #计算机视觉