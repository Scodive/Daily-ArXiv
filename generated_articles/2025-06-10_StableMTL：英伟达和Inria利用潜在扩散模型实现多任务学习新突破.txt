标题：StableMTL：英伟达和Inria利用潜在扩散模型实现多任务学习新突破


近年来，多任务学习（MTL）在计算机视觉领域备受关注，它能够同时解决多个相关任务，例如语义分割、深度估计和光流预测等。然而，现有MTL方法通常需要大量标注数据，限制了其在实际应用中的推广。为了解决这一难题，英伟达和Inria的研究人员提出了一种名为StableMTL的新方法，该方法利用潜在扩散模型（LDM）的强大生成能力，仅需在部分标注的合成数据集上进行训练，即可在多个真实世界场景中实现出色的零样本泛化性能。

**研究动机与背景**

传统MTL方法依赖于大量像素级别的标注数据，这在实际应用中往往是难以实现的。虽然部分标注的MTL方法有所发展，但它们通常受限于单个领域，泛化能力较弱，并且难以平衡不同任务之间的冲突。为了克服这些限制，StableMTL提出了一种新的训练范式，即利用多个部分标注的合成数据集进行训练。这种方法虽然降低了标注成本，但也引入了合成数据到真实数据的领域差距，因此需要模型具备强大的泛化能力。

**方法与技术亮点**

StableMTL的核心思想是重新利用预训练的LDM进行潜在回归。具体来说，该方法首先将图像和对应的任务标注编码到LDM的潜在空间中，然后训练一个UNet模型来预测任务标注的潜在表示。为了实现高效的任务间信息共享，StableMTL引入了一种多流架构，该架构包含一个主UNet和一个或多个辅助UNet。主UNet负责最终的预测，而辅助UNet则为每个任务生成特定的特征流。通过一个新颖的任务注意力机制，主UNet可以有效地利用来自辅助UNet的特征，从而实现任务间的知识迁移。此外，StableMTL采用了一种统一的潜在损失函数，避免了传统MTL方法中繁琐的损失函数权重调整。

**主要发现与成果**

StableMTL在7个任务和8个基准数据集上取得了显著的成果。实验结果表明，StableMTL在零样本设置下，能够有效地泛化到各种真实世界场景，并且在多个任务上优于现有的部分标注MTL方法。值得一提的是，StableMTL在整体多任务性能上实现了高达+83.54%的提升。

**意义与应用前景**

StableMTL的提出，为多任务学习提供了一种新的解决方案。通过利用LDM的生成能力和多流架构的任务注意力机制，StableMTL能够在部分标注的合成数据集上训练出具有强大泛化能力的模型。这项研究成果有望降低MTL的标注成本，推动其在自动驾驶、机器人和虚拟现实等领域的广泛应用。

标签：#多任务学习 #潜在扩散模型 #零样本学习 #计算机视觉 #英伟达 #Inria