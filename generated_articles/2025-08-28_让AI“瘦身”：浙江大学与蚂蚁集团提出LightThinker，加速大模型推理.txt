标题：让AI“瘦身”：浙江大学与蚂蚁集团提出LightThinker，加速大模型推理


大型语言模型（LLM）在处理复杂推理任务时表现出色，但其生成长文本时所需的海量内存和计算资源，如同“大象”般笨重，限制了其效率。近日，来自浙江大学与蚂蚁集团的研究团队提出了名为“LightThinker”的新方法，旨在让LLM在推理过程中“边走边压缩”，显著提升效率，而又不牺牲准确性。

**研究动机与背景：**

随着“思维链”（Chain-of-Thought, CoT）等方法的兴起，LLM的推理过程变得越来越细致，从“快速思考”转向“慢速思考”，以解决更复杂的问题。然而，这种“慢速思考”模式会产生大量的中间文本（tokens），导致Transformer架构中二次方增长的计算复杂度和线性增长的内存占用。特别是KV Cache，在长文本生成时会占用与模型本身相当的内存，严重制约了LLM在实际应用中的效率。现有方法要么需要复杂的提示工程，要么引入额外的推理延迟，都未能完美解决这一挑战。

**LightThinker的核心亮点：**

LightThinker的灵感来源于人类解决问题的方式：我们通常只记录关键步骤，而将大部分思考过程内化。该方法的核心在于训练LLM学会“何时”以及“如何”压缩中间思考过程。

具体而言，LightThinker通过数据重构，教会模型将冗长的思考步骤压缩成更紧凑的“要点”（gist tokens）。同时，它设计了特殊的注意力掩码（attention masks），引导模型在压缩时聚焦关键信息，并在压缩后基于这些要点继续推理。这种“压缩-继续”的模式，显著减少了存储在上下文窗口中的token数量，从而降低了内存和计算成本。

**关键技术与成果：**

为了量化压缩程度，研究人员还引入了“依赖性”（Dependency, Dep）指标，衡量生成每个token对历史token的依赖程度。实验结果表明，LightThinker在Qwen模型上，可以将峰值内存使用量降低70%，推理时间减少26%，同时准确率仅下降1%。在Llama模型上，虽然准确率下降稍多，但同样实现了显著的效率提升。与现有方法AnLLM相比，LightThinker通过解耦压缩与生成任务，并允许在压缩时访问更多上下文信息，表现出更好的性能。

**意义与应用前景：**

LightThinker的创新之处在于，它提供了一种在不牺牲模型性能的前提下，有效提升LLM推理效率的新途径。这项研究为解决LLM的“算力焦虑”提供了新的思路，有望加速LLM在各种需要复杂推理的场景中的应用，例如更流畅的对话系统、更高效的代码生成以及更智能的问答系统等。

#大模型 #AI推理 #计算效率 #浙江大学 #蚂蚁集团