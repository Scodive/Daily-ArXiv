标题：Meta V-JEPA 2：AI模型理解、预测和规划能力的新突破


人工智能领域面临的一大挑战是让AI通过观察学习理解世界并自主行动。Meta FAIR团队发布了题为“V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning”的研究论文，该研究探索了一种自监督学习方法，结合互联网规模的视频数据和少量交互数据（机器人轨迹），以开发能够理解、预测和在物理世界中进行规划的模型。这项研究为AI在现实世界中的应用开辟了新的可能性。

**研究动机与背景**

论文指出，人类能够适应新任务和不熟悉的环境，这得益于我们通过整合感官输入来构建世界内部模型，并预测未来状态。先前的研究主要依赖于状态-动作序列的交互数据，但真实世界交互数据的有限性限制了这些方法的可扩展性。V-JEPA 2旨在通过自监督学习，利用海量互联网视频数据，学习世界的背景知识，从而克服这一限制。

**方法与技术亮点**

V-JEPA 2的核心是基于Joint-Embedding Predictive Architecture (JEPA)，通过在学习到的表征空间中进行预测来学习。该方法与侧重于视频生成的传统方法不同，它专注于学习场景中可预测的方面（例如，运动物体的轨迹），而忽略生成目标强调的不可预测的细节。V-JEPA 2的训练过程分为两个阶段：
1.  **无动作预训练**：在超过100万小时的互联网视频和图像数据集上，使用掩码降噪特征预测目标进行预训练。模型预测视频中被遮蔽的部分，从而学习视频的表征。
2.  **动作条件后训练**：利用少量来自Droid数据集的机器人交互数据，训练一个潜在的动作条件世界模型V-JEPA 2-AC。该模型使用Transformer网络，根据动作和先前的状态自回归地预测下一帧的表征。

**主要发现与成果**

V-JEPA 2在多个任务上取得了显著成果：
*   **运动理解**：在Something-Something v2数据集上达到77.3%的top-1准确率。
*   **人类行为预测**：在Epic-Kitchens-100数据集上，recall-at-5指标达到39.7%，超越了之前的特定任务模型。
*   **视频问答**：与大型语言模型对齐后，在多个视频问答任务上达到最先进的性能，例如PerceptionTest (84.0) 和 TempCompass (76.9)。
*   **机器人规划**：V-JEPA 2-AC能够在Franka机械臂上实现零样本物体抓取和放置，无需收集新环境的数据，也无需特定任务的训练或奖励。

**意义与应用前景**

V-JEPA 2的成功表明，通过自监督学习和少量交互数据，可以构建一个能够理解物理世界、预测未来状态并在新环境中进行有效规划的世界模型。这项研究为开发更智能、更具适应性的AI系统奠定了基础，这些系统能够处理各种现实世界的任务，例如自动驾驶、机器人辅助和智能家居等。

标签：#人工智能 #视频理解 #自监督学习 #机器人规划 #Meta