标题：科学家首次认知考试：上海AI实验室发布SFE基准评测MLLM的认知能力


科学研究越来越依赖于复杂的多模态推理，它整合了信息密集的科学数据和特定领域的专业知识。科学多模态大语言模型（MLLM）拥有专家级的科学基准，有潜力显著增强实际工作流程中的科学发现过程。然而，当前的科学基准主要集中于评估 MLLM 的知识理解能力，导致对其感知和推理能力的评估不足。为了弥补这一差距，上海人工智能实验室的研究团队提出了科学家首次认知考试（SFE）基准，旨在通过三个认知水平评估 MLLM 的科学认知能力：科学信号感知、科学属性理解和科学比较推理。

SFE 包含跨五个高价值学科的 66 个多模态任务中的 830 个经过专家验证的 VQA 对。大量实验表明，当前最先进的 GPT-o3 和 InternVL-3 在 SFE 上的准确率分别仅为 34.08% 和 26.52%，这表明 MLLM 在科学领域仍有很大的改进空间。研究团队希望 SFE 中获得的见解将有助于人工智能增强科学发现的进一步发展。

该研究的动机源于现有科学基准的局限性。这些基准通常从学术材料和教科书等二级来源提取任务，未能充分考察分析科学数据所需的认知能力（例如，感知、理解和推理）。此外，这些基准往往只关注从数据中解释领域知识的单一能力，而忽略了从感知到推理的整个过程。

SFE 的方法与技术亮点在于其三层认知能力分类框架。L1 级的科学信号感知能力，旨在考察 MLLM 从科学原始数据可视化中辨别关键信息的能力。L2 级的科学属性理解能力，旨在考察 MLLM 解释领域专家知识的能力。L3 级的科学比较推理能力，旨在考察 MLLM 通过对多个科学可视化信息源进行结构化比较来推导现象学见解的能力。SFE 涵盖天文学、化学、地球科学、生命科学和材料科学五个学科，共计 66 个专家设计的多模态任务。每个任务都基于原始科学数据，并构建为视觉问答（VQA）对，旨在考察特定层次的科学认知能力。

该论文的主要发现与成果是，通过 SFE 对 16 个最先进的 MLLM 进行了基准测试，结果表明，尽管这些 MLLM 在 MMLU 和 ScienceQA 等基准测试中表现良好，但在 SFE 基准测试中均表现欠佳。这表明 SFE 是科学 MLLM 发展的一个具有挑战性的前沿领域。具体来说，GPT-o3 在 SFE 上的平均得分最高，为 34.08%，而其他模型的表现则参差不齐，表明 SFE 能够区分不同 MLLM 的能力。

SFE 的意义与应用前景在于，它为科学 MLLM 的发展提供了一个细粒度的评估框架。通过将科学任务按认知能力进行分类，SFE 能够对 MLLM 在科学研究不同层面的参与程度进行精细评估。SFE 的发布，为研究人员提供了一个全面的基准，用于评估和改进 MLLM 在科学领域的应用，从而促进人工智能驱动的科学发现。

标签：#人工智能 #多模态学习 #科学基准 #认知能力 #MLLM