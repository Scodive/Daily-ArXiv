标题：北大iFairy：让AI模型“瘦身”又“增智”的2比特复数魔法


随着大型语言模型（LLM）的飞速发展，其巨大的参数量和计算需求成为了制约其广泛应用的关键瓶颈。为了解决这一难题，研究人员一直在探索更高效的模型压缩技术。近日，北京大学的研究团队提出了一种名为iFairy的创新框架，它不仅将模型参数压缩至前所未有的2比特，更重要的是，通过引入复数运算，竟然还能提升模型的整体性能，这无疑为极低比特模型的研究开辟了新方向。

**打破“天花板”，从“减负”到“增效”**

传统模型压缩技术，如量化，通常是在已有的全精度模型基础上，努力减少量化误差，以期在低比特下尽可能接近全精度模型的表现。然而，全精度模型的准确度本身就构成了量化模型性能的“天花板”。iFairy的研究则另辟蹊径，它首先致力于提升全精度模型的“天花板”，即增强其表达能力，然后再将其高效地量化到2比特。这一思路的核心在于，一个更强大的全精度模型，即使经过极低比特量化，也能达到更高的最终精度。

**复数魔法：2比特蕴藏的无限可能**

iFairy的创新之处在于其大胆地将复数运算引入了LLM架构。复数（形如a+bi）比实数（a）拥有更丰富的表达空间，其相位信息可以为模型带来额外的表达能力，而无需增加参数量。iFairy的模型在核心的Transformer架构中，将所有参数和中间计算都转化为复数形式。

更关键的是，iFairy设计了一种独特的2比特量化方法——PhaseQuant。它将复数权重精确地映射到复平面上的四个“单位根”——{±1, ±i}。这种选择具有信息论上的最优性，因为它充分利用了2比特的全部表示能力，并且这些值具有完美的对称性。最令人惊叹的是，这些量化后的复数权重要么实部为零，要么虚部为零，这意味着在推理过程中，原本耗费计算资源的乘法运算被完全替换为更简单的加法和元素交换，极大地提高了计算效率。

**实验结果：性能与效率的双重飞跃**

研究人员通过实验验证了iFairy的强大实力。在语言建模任务上，iFairy在保持极低比特量的同时，其困惑度（Perplexity）得分显著优于现有的1.58比特模型（如BitNet b1.58），甚至接近了全精度FP16模型的水平。在下游任务的评估中，iFairy同样表现出色，其在零样本推理能力上甚至超越了同等规模的FP16模型。这充分证明了iFairy的复数量化方法不仅实现了极致的压缩，还有效地提升了模型的学习能力和泛化能力。

iFairy的出现，不仅展示了复数运算在AI模型压缩领域的巨大潜力，也为未来开发更高效、更强大的AI模型提供了新的思路和技术路径。

#iFairy #复数神经网络 #模型量化 #大型语言模型 #AI技术