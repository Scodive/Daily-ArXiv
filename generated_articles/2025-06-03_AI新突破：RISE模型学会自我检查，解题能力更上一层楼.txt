标题：AI新突破：RISE模型学会自我检查，解题能力更上一层楼


大型语言模型（LLMs）在复杂推理任务中展现出巨大潜力，而利用可验证奖励的强化学习（RLVR）是增强这些能力的关键策略。然而，模型常常陷入“表面自省”的困境，无法有效验证自身输出的正确性。为了解决这个问题，本文介绍了一种名为RISE（Reinforcing Reasoning with Self-Verification）的全新在线强化学习框架，它能够同步训练LLM，提升其解决问题和自我验证的能力。

**研究动机与背景**：
以往的研究表明，即使使用基于结果的强化学习，模型也可能在没有深入理解推理过程或具备强大的自我评估能力的情况下，生成正确的答案。这导致模型难以识别自身推理中的缺陷，并验证输出的正确性。虽然有些方法明确地结合了自我批评，但解决问题和验证解决方案的学习过程往往是分离的，或者缺乏对验证技能本身的直接反馈。

**方法与技术亮点**：
RISE的核心机制是利用来自结果验证器的可验证奖励，为解决方案生成和自我验证任务提供即时反馈。在每次迭代中，模型首先生成解决方案，然后使用预定义的模板，将问题和生成的解决方案格式化为验证提示，提示模型批判性地评估自己的解决方案并给出评分。用于评估问题解决方案的同一结果验证器也为验证任务提供ground truth监督。问题解决轨迹和自我验证轨迹，连同它们各自的可验证奖励，然后被组合起来，使用统一的强化学习目标来更新模型的参数。

**主要发现与成果**：
通过在各种数学推理基准上的大量实验表明，RISE能够持续提高模型解决问题的准确性，同时培养强大的自我验证技能。分析表明，在线验证具有优势，增加验证计算量也带来了好处。此外，RISE模型在推理过程中表现出更频繁和准确的自我验证行为。与仅包含问题解决监督的Zero-RL基线相比，RISE模型在所有模型尺寸上始终优于其Zero-RL对应模型，在推理和自我验证任务中都表现出色。

**意义与应用前景**：
RISE的优势在于其在线验证机制，以及增加验证训练计算量所带来的好处。所开发的自我验证能力有助于更准确和可靠的解决方案生成。RISE为构建更可靠和具有自我意识的LLM推理器提供了一个有希望的方向，它适用于各种策略梯度算法，并且可以扩展到其他具有可验证奖励的领域。未来的工作包括探索数学推理之外的其他具有挑战性的推理领域，例如代码生成、物理推理和现实世界推理密集型领域。

标签：#人工智能 #强化学习 #自我验证 #数学推理 #语言模型