标题：AI模型训练新发现：模仿专家不如自主探索


大型视觉语言模型（LVLMs）的训练通常遵循“监督微调（SFT）+强化学习（RL）”的范式。然而，这项研究对这一主流方法提出了质疑，揭示了一个关键问题：SFT可能会通过模仿专家模型引入“伪推理路径”，从而显著削弱后续RL的效果。这种模仿虽然能让模型看似具备推理能力，但往往导致冗长、犹豫、信息量不足，甚至错误的推理步骤。

为了深入研究这一现象，研究者们构建了一个新的多模态数据集VLAA-Thinking，专门用于支持LVLMs的推理。该数据集通过一个六步流程构建，包括图像描述、推理提炼、答案改写和验证，包含高质量的逐步视觉推理轨迹，适用于SFT，同时还有一个更具挑战性的RL分割。

研究团队通过大量实验，对比了SFT、RL及其组合的效果。结果表明，虽然SFT有助于模型学习推理格式，但它常常将模型锁定在模仿性的、僵化的推理模式中，阻碍了进一步学习。相反，基于Group Relative Policy Optimization (GRPO)，并结合感知和认知信号的新型混合奖励模块的RL方法，能够培养更真实、更具适应性的推理行为。值得注意的是，基于Qwen2.5VL 3B的VLAA-Thinker模型，在4B规模的LVLMs中，于Open LMM Reasoning Leaderboard上取得了第一名的成绩，超越了之前的最佳水平1.8%。

这项研究的主要发现包括：SFT会诱导“伪推理路径”，阻碍模型进行真正的推理；直接使用RL进行训练可以获得更有效和适应性强的思维模式；混合奖励机制能够有效地提升RL训练的效果。研究结果表明，在训练具备推理能力的LVLMs时，应更加重视RL的作用，避免过度依赖SFT。

这项研究为开发具备推理能力的LVLMs提供了宝贵的见解，并为未来的研究指明了方向。通过强调RL的重要性，并提出有效的训练方法，这项工作有望推动LVLMs在各个领域的应用，例如智能问答、图像理解和机器人控制等。

标签：#人工智能 #视觉语言模型 #强化学习 #监督学习 #推理能力