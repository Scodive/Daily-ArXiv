标题：MiMo-7B：小米发布推理能力至上的大语言模型，7B参数超越32B模型


MiMo-7B是小米LLM-Core团队发布的一系列专为推理任务设计的大语言模型。该模型在预训练和后训练阶段都进行了优化，旨在充分挖掘语言模型的推理潜力。通过增强数据预处理流程、采用三阶段数据混合策略，以及引入多Token预测目标，MiMo-7B-Base在25万亿tokens上进行了预训练，展现出卓越的推理潜力，甚至超越了更大的32B模型。经过强化学习（RL）微调后的MiMo-7B-RL在数学、代码和通用推理任务上表现出色，性能超越OpenAI o1-mini。

**研究动机与背景**

当前，具备高级推理能力的大语言模型在数学推理和代码生成等复杂任务中取得了显著成果。这些模型通过大规模强化学习，形成了复杂的推理模式，例如逐步分析、自我反思和回溯，从而在不同领域实现更强大、更准确的问题解决能力。然而，大多数成功的RL研究依赖于较大的基础模型，例如32B模型，尤其是在增强代码推理能力方面。MiMo-7B的研究团队认为，RL训练的推理模型的有效性取决于基础模型固有的推理潜力。

**方法与技术亮点**

MiMo-7B的创新之处在于其在预训练和后训练阶段的优化策略：

*   **预训练阶段**：优化数据预处理流程，增强文本提取工具，应用多维度数据过滤，增加预训练数据中推理模式的密度。同时，采用多种策略生成大规模、多样化的合成推理数据。采用三阶段数据混合策略，在约25万亿tokens上进行预训练。引入多Token预测（MTP）作为额外的训练目标，提高模型性能并加速推理。MTP模块允许模型预先规划并生成表示，从而促进更准确、更快速的未来token预测。在推理阶段，多个并行的MTP层可以通过推测解码来显著加速推理。
*   **后训练阶段**：构建包含13万个可验证的数学和编程问题的强化学习训练数据集。引入测试难度驱动的代码奖励机制，缓解稀疏奖励问题。实施数据重采样策略，提高rollout抽样效率，稳定训练。测试难度驱动的奖励机制，灵感来源于国际信息学奥林匹克竞赛（IOI）的评分规则，通过为不同难度级别的测试用例分配精细的分数，可以更有效地通过密集奖励信号优化策略。

**主要发现与成果**

*   MiMo-7B-Base在通用知识和编码任务中优于同等规模的最先进开源模型。在BBH基准测试中，MiMo-7B-Base获得了75.2分，展示了卓越的推理能力。其在SuperGPQA上的出色表现进一步突显了其处理复杂研究生级别问题的能力。
*   MiMo-7B-RL-Zero在数学和代码任务上的RL训练性能超过了32B基础模型。
*   MiMo-7B-RL实现了出色的推理性能。在AIME 2025上，MiMo-7B-RL获得了55.4分，超过o1-mini 4.7分。在算法代码生成任务中，MiMo-7B-RL表现出色，在LiveCodeBench v5和最新的v6上均显著优于OpenAI o1-mini。

**意义与应用前景**

MiMo-7B的发布为开发更强大的推理模型提供了有价值的见解。其在预训练和后训练阶段的创新策略，以及高效的RL训练框架，为未来的研究提供了新的方向。MiMo-7B在数学、代码和通用推理任务上的卓越表现，使其在教育、科研、软件开发等领域具有广阔的应用前景。

标签：#大语言模型 #推理能力 #强化学习 #MiMo-7B #小米