标题：清华大学提出STAIR框架：用内省推理提升大语言模型的安全对齐


近年来，大型语言模型（LLMs）在各种任务中表现出强大的能力，但也暴露出潜在的生成有害内容的风险。如何确保LLMs的安全性和无害性，已成为一个重要的研究方向。然而，现有的安全对齐方法往往面临安全性和性能之间的权衡，并且容易受到对抗性攻击。清华大学等机构的研究者们提出了STAIR框架，旨在通过内省推理来提升LLMs的安全对齐，相关资源已开源。

**研究动机与背景**

现有的安全对齐方法通常依赖于直接拒绝恶意查询，但这种“浅层对齐”容易被各种jailbreak攻击绕过。这些攻击通过精心设计的提示，掩盖潜在的危害，诱导模型生成有害内容。为了解决这个问题，研究者们希望赋予LLMs“System 2 thinking”的能力，即通过更深入的思考和逻辑推理，识别潜在的风险，从而做出更安全的响应。

**方法与技术亮点**

STAIR框架包含三个阶段：结构化CoT格式对齐、基于安全感知的蒙特卡洛树搜索（SI-MCTS）的自我改进、以及测试时缩放。

1.  **结构化CoT格式对齐**：首先，通过在少量安全和helpfulness数据上进行微调，使模型具备结构化的思维链（CoT）推理能力。模型被要求按照特定的格式输出每一步的推理过程，包括步骤标题和详细思考，并用特殊token进行标记，从而增强推理过程的可解释性。
2.  **基于SI-MCTS的自我改进**：为了进一步提升模型的安全感知推理能力，研究者们提出了一种新颖的SI-MCTS方法。该方法在传统的MCTS基础上，引入了安全感知的奖励函数，将安全相关的信息融入到内部搜索节点中，从而引导模型探索更安全的推理路径。此外，研究者还设计了一个自奖励机制，利用模型自身的能力对响应进行评估，避免了对外部评估器的依赖。
3.  **测试时缩放**：在推理阶段，采用测试时缩放技术，通过分配额外的计算资源，利用诸如Best-of-N或Beam Search等高级搜索算法，促使模型进行更深入的思考，从而生成更高质量的响应。

**主要发现与成果**

实验结果表明，STAIR框架能够有效缓解有害输出，同时更好地保持helpfulness。在StrongReject基准测试中，STAIR框架在LLaMA模型上实现了0.88的goodness score，优于最佳基线0.15。此外，STAIR框架还在helpfulness、truthfulness、robustness和privacy awareness等多个维度上取得了显著提升。通过测试时缩放，STAIR框架在安全性方面达到了与Claude-3.5相当的水平。

**意义与应用前景**

STAIR框架通过引入内省推理，为LLMs的安全对齐提供了一种新的思路。该框架不仅能够提升模型抵御对抗性攻击的能力，还能够在安全性和性能之间取得更好的平衡。STAIR框架的成功，为开发更安全、更可靠的LLMs奠定了基础，并有望在医疗诊断、教育工具和法律咨询等高风险领域得到广泛应用。

标签：#大语言模型 #安全对齐 #内省推理 #蒙特卡洛树搜索 #清华大学