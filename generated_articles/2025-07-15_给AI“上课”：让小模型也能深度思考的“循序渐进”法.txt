标题：给AI“上课”：让小模型也能深度思考的“循序渐进”法


想象一下，我们想教会一个孩子解决复杂的数学题。直接给他一道高难度奥数题，他很可能无从下手。但如果从简单的加减乘除开始，再逐步过渡到代数、几何，最后才挑战奥数，效果就会好很多。最近，来自德克萨斯农工大学的研究人员就将这一“循序渐进”的学习理念应用到了大型语言模型（LLM）的训练中，并取得了令人瞩目的成果。

这项研究发表在arXiv上，论文标题为《Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning》。它针对的是当前大型语言模型在进行复杂推理任务时遇到的瓶颈。虽然像DeepSeek-R1这样的模型已经展示了在数学和编程方面的推理能力，但研究发现，仅靠强化学习（RL）来提升模型处理困难任务的效果并不理想，因为困难任务的奖励信号稀疏，模型难以从中学习。

为了解决这个问题，研究人员借鉴了“课程学习”（Curriculum Learning）的思路，提出了一种名为“Easy2 Hard Reasoner”（E2H）的方法。简单来说，就是将一个复杂的推理任务分解成一系列由易到难的子任务。模型首先在简单的任务上进行训练，学习基础的推理技能，然后逐步接触更具挑战性的任务，从而构建起坚实的推理能力。这种方法就像是为AI量身定制了一套“学习计划”，让它能够有条不紊地进步。

E2H方法的核心亮点在于其创新的任务调度策略。研究人员发现，仅仅从易到难地顺序学习可能会导致模型遗忘早期学到的知识，或者过度拟合简单的任务。为了克服这些问题，他们设计了两种调度策略：一种是“余弦调度”（Cosine Scheduling），通过余弦函数平滑地调整模型对不同难度任务的关注度；另一种是“高斯调度”（Gaussian Scheduling），借鉴高斯混合模型，更精细地控制模型在不同学习阶段的样本采样比例，以避免对简单任务的过度拟合。

研究人员还从理论上分析了E2H的有效性，证明了在合适的任务分解和调度下，课程学习方法比直接在困难任务上训练需要更少的样本，并且能够保证模型的收敛性。

实验结果也印证了E2H的强大威力。研究人员在多种推理任务上测试了包括Qwen和LLaMA在内的多种小型语言模型（参数量在1.5B到3B之间）。结果显示，使用E2H方法训练后，这些小型模型在原本难以解决的复杂任务上表现出了显著的提升，甚至在一些“开箱即用”时表现不佳的任务上，也能达到与更大型模型相媲美的推理能力。特别是在处理数学推理和规划类任务时，E2H方法展现出了优异的性能，尤其是在测试模型泛化能力的“开箱即用”（OOD）场景下。

这项研究的意义重大，它不仅为提升小型语言模型的推理能力提供了一条切实可行的途径，也为理解和改进AI的学习机制提供了新的视角。未来，E2H方法有望应用于更多领域，让AI在处理复杂问题时能够像人类一样“举一反三”，展现出更强的智能。

#大型语言模型 #强化学习 #课程学习 #AI推理 #深度学习