#!/usr/bin/env python3
"""
æ¯æ—¥è‡ªåŠ¨è®ºæ–‡å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼šè‡ªåŠ¨è·å–ArXivæœ€æ–°è®ºæ–‡ï¼Œç”Ÿæˆç§‘æ™®æ–‡ç« å¹¶ä¿å­˜åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
python daily_auto.py

å¯é€‰å‚æ•°ï¼š
--count N    å¤„ç†Nç¯‡è®ºæ–‡ (é»˜è®¤3)
--category X æŒ‡å®šåˆ†ç±» (é»˜è®¤cs.AI)
--force      å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„è®ºæ–‡
"""

import requests
import sys
import argparse
from datetime import datetime, timedelta
import time
import random
import re
from arxiv import (
    download_pdf, 
    extract_text_from_pdf, 
    generate_xiaohongshu_post,
    sanitize_filename,
    insert_article_to_database,
    extract_arxiv_id_from_url
)
import os

def get_today_arxiv_papers_from_rss(categories=["cs.AI"]):
    """
    ä»ArXiv RSS feedè·å–ä»Šå¤©çš„æ–°è®ºæ–‡
    
    Args:
        categories: åˆ†ç±»åˆ—è¡¨ (å¦‚ ["cs.AI", "cs.CV", "cs.CL"])
    
    Returns:
        list: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
    """
    print(f"ğŸ” æ­£åœ¨ä»ArXiv RSSè·å–ä»Šå¤©çš„è®ºæ–‡ï¼Œåˆ†ç±»: {categories}")
    
    all_papers = []
    
    # ArXiv RSS URLæ ¼å¼ï¼šhttps://rss.arxiv.org/rss/{category}
    for category in categories:
        print(f"ğŸ“š æ­£åœ¨è·å– {category} åˆ†ç±»çš„RSS...")
        
        try:
            rss_url = f"https://rss.arxiv.org/rss/{category}"
            print(f"   RSS URL: {rss_url}")
            
            response = requests.get(rss_url, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS XML
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            category_papers = []
            
            # RSSä¸­çš„itemå…ƒç´ åŒ…å«è®ºæ–‡ä¿¡æ¯
            for item in root.findall('.//item'):
                try:
                    # è·å–æ ‡é¢˜
                    title_elem = item.find('title')
                    if title_elem is None:
                        continue
                    
                    clean_title = title_elem.text.strip()
                    
                    # è·å–é“¾æ¥å’ŒArXiv ID
                    link_elem = item.find('link')
                    if link_elem is None:
                        continue
                    
                    abs_url = link_elem.text.strip()
                    # ä»é“¾æ¥ä¸­æå–ArXiv IDï¼šhttps://arxiv.org/abs/2506.05352
                    arxiv_match = re.search(r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}(?:v\d+)?)', abs_url)
                    if not arxiv_match:
                        continue
                    
                    arxiv_id = arxiv_match.group(1)
                    
                    # ä»abstracté“¾æ¥æ„é€ PDFé“¾æ¥
                    pdf_url = abs_url.replace('/abs/', '/pdf/') + '.pdf'
                    
                    # è·å–æè¿°ï¼ˆæ‘˜è¦ï¼‰
                    desc_elem = item.find('description')
                    summary = ""
                    if desc_elem is not None and desc_elem.text:
                        desc_text = desc_elem.text.strip()
                        
                        # æå–æ‘˜è¦éƒ¨åˆ†ï¼šåœ¨"Abstract:"ä¹‹åçš„å†…å®¹
                        abstract_match = re.search(r'Abstract:\s*(.+)', desc_text, re.DOTALL)
                        if abstract_match:
                            summary = abstract_match.group(1).strip()
                        else:
                            summary = desc_text
                        
                        # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
                        summary = re.sub(r'\s+', ' ', summary)
                    
                    # è·å–å‘å¸ƒæ—¥æœŸ
                    pub_date_elem = item.find('pubDate')
                    pub_date = datetime.now()  # é»˜è®¤ä¸ºå½“å‰æ—¶é—´
                    if pub_date_elem is not None:
                        try:
                            # RSSæ—¥æœŸæ ¼å¼ï¼šMon, 09 Jun 2025 00:00:00 -0400
                            from email.utils import parsedate_to_datetime
                            pub_date = parsedate_to_datetime(pub_date_elem.text)
                        except:
                            pub_date = datetime.now()
                    
                    paper_info = {
                        'title': clean_title,
                        'pdf_url': pdf_url,
                        'arxiv_id': arxiv_id,
                        'published': pub_date,
                        'categories': [category],
                        'primary_category': category,
                        'summary': summary[:500] + '...' if len(summary) > 500 else summary
                    }
                    
                    category_papers.append(paper_info)
                    
                except Exception as e:
                    print(f"   âš ï¸  è§£æè®ºæ–‡æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            print(f"   âœ… {category} åˆ†ç±»æ‰¾åˆ° {len(category_papers)} ç¯‡è®ºæ–‡")
            all_papers.extend(category_papers)
            
        except Exception as e:
            print(f"   âŒ è·å– {category} RSSå¤±è´¥: {e}")
    
    # å»é‡ï¼ˆæŒ‰ArXiv IDï¼‰
    unique_papers = {}
    for paper in all_papers:
        arxiv_id = paper['arxiv_id']
        if arxiv_id not in unique_papers:
            unique_papers[arxiv_id] = paper
    
    result_papers = list(unique_papers.values())
    print(f"âœ… æ€»å…±æ‰¾åˆ° {len(result_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    return result_papers

def get_today_arxiv_papers_by_category_api(categories=["cs.AI"], max_results_per_category=100):
    """
    ä½¿ç”¨æ”¹è¿›çš„APIæŸ¥è¯¢è·å–æœ€è¿‘æäº¤çš„ArXivè®ºæ–‡
    
    Args:
        categories: åˆ†ç±»åˆ—è¡¨
        max_results_per_category: æ¯ä¸ªåˆ†ç±»çš„æœ€å¤§ç»“æœæ•°
    
    Returns:
        list: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
    """
    print(f"ğŸ” æ­£åœ¨ä½¿ç”¨APIè·å–æœ€æ–°ArXivè®ºæ–‡ï¼Œåˆ†ç±»: {categories}")
    
    all_papers = []
    
    for category in categories:
        print(f"ğŸ“š æ­£åœ¨è·å– {category} åˆ†ç±»çš„è®ºæ–‡...")
        
        try:
            # ArXiv API URL - ä¸ç”¨æ—¥æœŸè¿‡æ»¤ï¼Œè·å–æœ€æ–°çš„è®ºæ–‡
            base_url = "http://export.arxiv.org/api/query"
            
            # ç®€åŒ–æŸ¥è¯¢ - åªæŒ‰åˆ†ç±»ï¼ŒæŒ‰æœ€æ–°æ’åº
            query = f"cat:{category}"
            params = {
                "search_query": query,
                "start": 0,
                "max_results": max_results_per_category,
                "sortBy": "submittedDate",
                "sortOrder": "descending"
            }
            
            print(f"   æŸ¥è¯¢: {query}")
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            category_papers = []
            today = datetime.now().date()
            
            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                # æå–è®ºæ–‡ä¿¡æ¯
                title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
                title = re.sub(r'\s+', ' ', title)  # æ¸…ç†å¤šä½™ç©ºæ ¼
                
                # è·å–PDFé“¾æ¥
                pdf_link = None
                for link in entry.findall('{http://www.w3.org/2005/Atom}link'):
                    if link.get('type') == 'application/pdf':
                        pdf_link = link.get('href')
                        break
                
                # è·å–å‘å¸ƒæ—¥æœŸ
                published = entry.find('{http://www.w3.org/2005/Atom}published').text
                pub_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                
                # åªä¿ç•™ä»Šå¤©æˆ–æœ€è¿‘1-2å¤©çš„è®ºæ–‡
                days_diff = (datetime.now().date() - pub_date.date()).days
                if days_diff > 2:  # è¶…è¿‡2å¤©çš„è·³è¿‡
                    continue
                
                # è·å–ArXiv ID
                arxiv_id = entry.find('{http://www.w3.org/2005/Atom}id').text
                arxiv_id = arxiv_id.split('/')[-1]  # æå–IDéƒ¨åˆ†
                
                # è·å–åˆ†ç±»ä¿¡æ¯
                categories_list = []
                for category_elem in entry.findall('{http://www.w3.org/2005/Atom}category'):
                    categories_list.append(category_elem.get('term'))
                
                # è·å–æ‘˜è¦
                summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
                summary = re.sub(r'\s+', ' ', summary)  # æ¸…ç†å¤šä½™ç©ºæ ¼
                
                paper_info = {
                    'title': title,
                    'pdf_url': pdf_link,
                    'arxiv_id': arxiv_id,
                    'published': pub_date,
                    'categories': categories_list,
                    'primary_category': category,
                    'summary': summary[:500] + '...' if len(summary) > 500 else summary
                }
                
                category_papers.append(paper_info)
            
            print(f"   âœ… {category} åˆ†ç±»æ‰¾åˆ° {len(category_papers)} ç¯‡æœ€è¿‘è®ºæ–‡")
            all_papers.extend(category_papers)
            
        except Exception as e:
            print(f"   âŒ è·å– {category} åˆ†ç±»è®ºæ–‡å¤±è´¥: {e}")
    
    # å»é‡ï¼ˆæŒ‰ArXiv IDï¼‰
    unique_papers = {}
    for paper in all_papers:
        arxiv_id = paper['arxiv_id']
        if arxiv_id not in unique_papers:
            unique_papers[arxiv_id] = paper
    
    result_papers = list(unique_papers.values())
    print(f"âœ… æ€»å…±æ‰¾åˆ° {len(result_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    return result_papers

def get_recent_arxiv_papers_fallback(categories=["cs.AI"], max_results=100, days_back=3):
    """
    åå¤‡æ–¹æ¡ˆï¼šè·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ï¼ˆå¦‚æœä»Šå¤©æ²¡æœ‰è®ºæ–‡ï¼‰
    
    Args:
        categories: åˆ†ç±»åˆ—è¡¨
        max_results: æœ€å¤§ç»“æœæ•°
        days_back: å‘å‰æŸ¥æ‰¾å¤©æ•°
    
    Returns:
        list: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
    """
    print(f"ğŸ”„ ä½¿ç”¨åå¤‡æ–¹æ¡ˆï¼šè·å–æœ€è¿‘{days_back}å¤©çš„è®ºæ–‡...")
    
    all_papers = []
    
    for category in categories:
        try:
            base_url = "http://export.arxiv.org/api/query"
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            query = f"cat:{category}"
            params = {
                "search_query": query,
                "start": 0,
                "max_results": max_results,
                "sortBy": "submittedDate",
                "sortOrder": "descending"
            }
            
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                # è·å–å‘å¸ƒæ—¥æœŸ
                published = entry.find('{http://www.w3.org/2005/Atom}published').text
                pub_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                
                # åªä¿ç•™æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
                if pub_date >= start_date.replace(tzinfo=pub_date.tzinfo):
                    # æå–è®ºæ–‡ä¿¡æ¯
                    title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
                    title = re.sub(r'\s+', ' ', title)
                    
                    # è·å–PDFé“¾æ¥
                    pdf_link = None
                    for link in entry.findall('{http://www.w3.org/2005/Atom}link'):
                        if link.get('type') == 'application/pdf':
                            pdf_link = link.get('href')
                            break
                    
                    # è·å–ArXiv ID
                    arxiv_id = entry.find('{http://www.w3.org/2005/Atom}id').text
                    arxiv_id = arxiv_id.split('/')[-1]
                    
                    # è·å–åˆ†ç±»ä¿¡æ¯
                    categories_list = []
                    for category_elem in entry.findall('{http://www.w3.org/2005/Atom}category'):
                        categories_list.append(category_elem.get('term'))
                    
                    # è·å–æ‘˜è¦
                    summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
                    summary = re.sub(r'\s+', ' ', summary)
                    
                    all_papers.append({
                        'title': title,
                        'pdf_url': pdf_link,
                        'arxiv_id': arxiv_id,
                        'published': pub_date,
                        'categories': categories_list,
                        'primary_category': category,
                        'summary': summary[:500] + '...' if len(summary) > 500 else summary
                    })
            
        except Exception as e:
            print(f"âŒ è·å– {category} åˆ†ç±»è®ºæ–‡å¤±è´¥: {e}")
    
    # å»é‡
    unique_papers = {}
    for paper in all_papers:
        arxiv_id = paper['arxiv_id']
        if arxiv_id not in unique_papers:
            unique_papers[arxiv_id] = paper
    
    result_papers = list(unique_papers.values())
    print(f"âœ… åå¤‡æ–¹æ¡ˆæ‰¾åˆ° {len(result_papers)} ç¯‡è®ºæ–‡")
    return result_papers

def check_paper_exists_in_db(arxiv_id, title):
    """æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“"""
    try:
        from arxiv import connect_to_database
        conn = connect_to_database()
        if not conn:
            return False
        
        cursor = conn.cursor()
        
        if arxiv_id:
            cursor.execute("SELECT id FROM articles WHERE arxiv_id = %s", (arxiv_id,))
        else:
            cursor.execute("SELECT id FROM articles WHERE title = %s", (title,))
        
        exists = cursor.fetchone() is not None
        
        cursor.close()
        conn.close()
        
        return exists
        
    except Exception as e:
        print(f"âš ï¸  æ£€æŸ¥æ•°æ®åº“å¤±è´¥: {e}")
        return False

def process_single_paper(paper_info, force_process=False):
    """
    å¤„ç†å•ç¯‡è®ºæ–‡
    
    Args:
        paper_info: è®ºæ–‡ä¿¡æ¯å­—å…¸
        force_process: æ˜¯å¦å¼ºåˆ¶å¤„ç†å·²å­˜åœ¨çš„è®ºæ–‡
    
    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    title = paper_info['title']
    pdf_url = paper_info['pdf_url']
    arxiv_id = paper_info['arxiv_id']
    
    print(f"\nğŸ“„ æ­£åœ¨å¤„ç†è®ºæ–‡: {title[:60]}...")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé™¤éå¼ºåˆ¶å¤„ç†ï¼‰
    if not force_process and check_paper_exists_in_db(arxiv_id, title):
        print(f"âš ï¸  è®ºæ–‡å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
        return True
    
    try:
        # ä¸‹è½½PDF
        pdf_data = download_pdf(pdf_url)
        if not pdf_data:
            print("âŒ PDFä¸‹è½½å¤±è´¥")
            return False
        
        # æå–æ–‡æœ¬
        extracted_text = extract_text_from_pdf(pdf_data)
        if not extracted_text or len(extracted_text.strip()) < 100:
            print("âŒ PDFæ–‡æœ¬æå–å¤±è´¥æˆ–å†…å®¹å¤ªå°‘")
            return False
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_text)} å­—ç¬¦çš„æ–‡æœ¬")
        
        # ç”Ÿæˆç§‘æ™®æ–‡ç« 
        generated_content = generate_xiaohongshu_post(extracted_text)
        if not generated_content:
            print("âŒ ç§‘æ™®æ–‡ç« ç”Ÿæˆå¤±è´¥")
            return False
        
        print("âœ… ç§‘æ™®æ–‡ç« ç”ŸæˆæˆåŠŸ")
        
        # è§£æç”Ÿæˆçš„å†…å®¹
        lines = generated_content.split('\n')
        article_title = title  # é»˜è®¤ä½¿ç”¨åŸæ ‡é¢˜
        article_body = []
        tags_line = ""
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„æ ‡é¢˜
        if lines and lines[0].startswith("æ ‡é¢˜ï¼š"):
            article_title = lines[0].replace("æ ‡é¢˜ï¼š", "").strip()
            content_start_index = 1
        else:
            content_start_index = 0
        
        # åˆ†ç¦»æ­£æ–‡å’Œæ ‡ç­¾
        for i in range(content_start_index, len(lines)):
            if lines[i].startswith("æ ‡ç­¾ï¼š"):
                tags_line = lines[i]
                break
            article_body.append(lines[i])
        
        if not tags_line and article_body and article_body[-1].strip().startswith("#"):
            tags_line = article_body.pop()
        
        final_article_text = "\n".join(article_body)
        if tags_line:
            final_article_text += "\n" + tags_line
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        current_date = datetime.now().strftime("%Y-%m-%d")
        sanitized_title = sanitize_filename(article_title)
        if not sanitized_title:
            sanitized_title = f"arxiv_{arxiv_id}"
        
        txt_filename = f"{current_date}_{sanitized_title}.txt"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "generated_articles"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        file_path = os.path.join(output_dir, txt_filename)
        
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(f"æ ‡é¢˜ï¼š{article_title}\n\n")
                f.write(final_article_text)
            print(f"ğŸ“„ æ–‡ç« å·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        content_for_db = final_article_text
        tags_for_db = tags_line.replace("æ ‡ç­¾ï¼š", "").strip() if tags_line else ""
        date_processed = datetime.now().date()
        
        db_success = insert_article_to_database(
            title=article_title,
            content=content_for_db,
            tags=tags_for_db,
            arxiv_id=arxiv_id,
            pdf_url=pdf_url,
            filename=txt_filename,
            date_processed=date_processed
        )
        
        if db_success:
            print("ğŸ‰ è®ºæ–‡å¤„ç†å®Œæˆï¼")
            return True
        else:
            print("âš ï¸  æ–‡ä»¶å·²ä¿å­˜ï¼Œä½†æ•°æ®åº“ä¿å­˜å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ å¤„ç†è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='æ¯æ—¥è‡ªåŠ¨è®ºæ–‡å¤„ç†è„šæœ¬')
    parser.add_argument('--count', type=int, default=5, help='å¤„ç†è®ºæ–‡æ•°é‡ (é»˜è®¤5)')
    parser.add_argument('--categories', nargs='+', default=['cs.AI'], 
                       help='ArXivåˆ†ç±»åˆ—è¡¨ (é»˜è®¤cs.AI, å¯æŒ‡å®šå¤šä¸ªå¦‚: cs.AI cs.CV cs.CL)')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„è®ºæ–‡')
    parser.add_argument('--fallback-days', type=int, default=3, help='åå¤‡æ–¹æ¡ˆæŸ¥æ‰¾å¤©æ•° (é»˜è®¤3)')
    
    args = parser.parse_args()
    
    print("ğŸ¤– æ¯æ—¥ArXivè®ºæ–‡è‡ªåŠ¨å¤„ç†è„šæœ¬å¯åŠ¨")
    print(f"ğŸ“Š é…ç½®: åˆ†ç±»={args.categories}, æ•°é‡={args.count}, å¼ºåˆ¶={args.force}")
    print("-" * 60)
    
    # å°è¯•å¤šç§æ–¹æ³•è·å–è®ºæ–‡
    papers = []
    
    # æ–¹æ³•1ï¼šRSS feed (ä¼˜å…ˆï¼Œå› ä¸ºRSSæ€»æ˜¯åŒ…å«æœ€æ–°è®ºæ–‡)
    print("ğŸ”„ æ–¹æ³•1: å°è¯•ä»RSS feedè·å–è®ºæ–‡...")
    try:
        papers = get_today_arxiv_papers_from_rss(categories=args.categories)
        if papers:
            print(f"âœ… RSSæ–¹æ³•æˆåŠŸè·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        else:
            print("âš ï¸ RSSæ–¹æ³•æœªè·å–åˆ°è®ºæ–‡")
    except Exception as e:
        print(f"âŒ RSSæ–¹æ³•å¤±è´¥: {e}")
    
    # æ–¹æ³•2ï¼šAPIæŸ¥è¯¢ (å¦‚æœRSSå¤±è´¥æˆ–ç»“æœå¤ªå°‘)
    if len(papers) < 5:
        print("ğŸ”„ æ–¹æ³•2: å°è¯•APIæŸ¥è¯¢è·å–æ›´å¤šè®ºæ–‡...")
        try:
            api_papers = get_today_arxiv_papers_by_category_api(
                categories=args.categories,
                max_results_per_category=20
            )
            
            # åˆå¹¶ç»“æœï¼Œå»é‡
            existing_ids = set(p['arxiv_id'] for p in papers)
            for paper in api_papers:
                if paper['arxiv_id'] not in existing_ids:
                    papers.append(paper)
                    existing_ids.add(paper['arxiv_id'])
            
            print(f"âœ… åˆå¹¶åæ€»å…±æœ‰ {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"âŒ APIæ–¹æ³•å¤±è´¥: {e}")
    
    # æ–¹æ³•3ï¼šåå¤‡æ–¹æ¡ˆ (å¦‚æœå‰ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥)
    if len(papers) < 2:
        print("ğŸ”„ æ–¹æ³•3: ä½¿ç”¨åå¤‡æ–¹æ¡ˆ...")
        try:
            fallback_papers = get_recent_arxiv_papers_fallback(
                categories=args.categories,
                max_results=args.count * 5,  # è·å–æ›´å¤šä»¥é˜²å¤„ç†å¤±è´¥
                days_back=args.fallback_days
            )
            
            existing_ids = set(p['arxiv_id'] for p in papers)
            for paper in fallback_papers:
                if paper['arxiv_id'] not in existing_ids:
                    papers.append(paper)
                    existing_ids.add(paper['arxiv_id'])
            
            print(f"âœ… æœ€ç»ˆæ€»å…±æœ‰ {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"âŒ åå¤‡æ–¹æ¡ˆå¤±è´¥: {e}")
    
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„è®ºæ–‡")
        return
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    papers.sort(key=lambda x: x['published'], reverse=True)
    
    print(f"\nğŸ“š å‡†å¤‡å¤„ç† {len(papers)} ç¯‡å€™é€‰è®ºæ–‡ï¼Œç›®æ ‡å¤„ç† {args.count} ç¯‡")
    
    # å¤„ç†è®ºæ–‡
    success_count = 0
    total_processed = 0
    processed_categories = set()
    
    for i, paper in enumerate(papers):
        if total_processed >= args.count:
            break
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†è¿›åº¦: {total_processed + 1}/{args.count}")
        print(f"ğŸ“„ è®ºæ–‡: {paper['title'][:60]}...")
        print(f"ğŸ·ï¸  åˆ†ç±»: {', '.join(paper.get('categories', [paper.get('primary_category', 'unknown')]))}")
        print(f"ğŸ†” ArXiv ID: {paper['arxiv_id']}")
        print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {paper['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        
        success = process_single_paper(paper, args.force)
        total_processed += 1
        
        if success:
            success_count += 1
            processed_categories.add(paper.get('primary_category', 'unknown'))
        
        # åœ¨å¤„ç†ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«è¯·æ±‚API
        if i < len(papers) - 1 and total_processed < args.count:
            delay = random.randint(8, 20)
            print(f"â° ç­‰å¾… {delay} ç§’åå¤„ç†ä¸‹ä¸€ç¯‡...")
            time.sleep(delay)
    
    print(f"\nğŸ¯ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡å¤„ç†: {total_processed} ç¯‡")
    print(f"âœ… æˆåŠŸ: {success_count} ç¯‡")
    print(f"âŒ å¤±è´¥: {total_processed - success_count} ç¯‡")
    print(f"ğŸ·ï¸  æ¶‰åŠåˆ†ç±»: {', '.join(processed_categories)}")
    
    if success_count > 0:
        print("\nğŸ’¾ æ–°å¤„ç†çš„è®ºæ–‡å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œç½‘ç«™å°†æ˜¾ç¤ºæœ€æ–°å†…å®¹ï¼")

if __name__ == "__main__":
    main() 